{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "UNet3Plus.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:01.434716Z",
          "iopub.execute_input": "2021-07-13T23:39:01.435079Z",
          "iopub.status.idle": "2021-07-13T23:39:01.485316Z",
          "shell.execute_reply.started": "2021-07-13T23:39:01.435048Z",
          "shell.execute_reply": "2021-07-13T23:39:01.484389Z"
        },
        "trusted": true,
        "id": "RPIHjuB-EHSM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gBqKbB6EHSS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:01.526218Z",
          "iopub.execute_input": "2021-07-13T23:39:01.526524Z",
          "iopub.status.idle": "2021-07-13T23:39:01.542283Z",
          "shell.execute_reply.started": "2021-07-13T23:39:01.526497Z",
          "shell.execute_reply": "2021-07-13T23:39:01.541227Z"
        },
        "trusted": true,
        "id": "19UouHmDEHST"
      },
      "source": [
        "from tensorflow import math\n",
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "def gelu_(X):\n",
        "\n",
        "    return 0.5*X*(1.0 + math.tanh(0.7978845608028654*(X + 0.044715*math.pow(X, 3))))\n",
        "\n",
        "def snake_(X, beta):\n",
        "\n",
        "    return X + (1/beta)*math.square(math.sin(beta*X))\n",
        "\n",
        "\n",
        "class GELU(Layer):\n",
        "    '''\n",
        "    Gaussian Error Linear Unit (GELU), an alternative of ReLU\n",
        "    \n",
        "    Y = GELU()(X)\n",
        "    \n",
        "    ----------\n",
        "    Hendrycks, D. and Gimpel, K., 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.\n",
        "    \n",
        "    Usage: use it as a tf.keras.Layer\n",
        "    \n",
        "    \n",
        "    '''\n",
        "    def __init__(self, trainable=False, **kwargs):\n",
        "        super(GELU, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(GELU, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        return gelu_(inputs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'trainable': self.trainable}\n",
        "        base_config = super(GELU, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    \n",
        "class Snake(Layer):\n",
        "    '''\n",
        "    Snake activation function $X + (1/b)*sin^2(b*X)$. Proposed to learn periodic targets.\n",
        "    \n",
        "    Y = Snake(beta=0.5, trainable=False)(X)\n",
        "    \n",
        "    ----------\n",
        "    Ziyin, L., Hartwig, T. and Ueda, M., 2020. Neural networks fail to learn periodic functions \n",
        "    and how to fix it. arXiv preprint arXiv:2006.08195.\n",
        "    \n",
        "    '''\n",
        "    def __init__(self, beta=0.5, trainable=False, **kwargs):\n",
        "        super(Snake, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.beta = beta\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.beta_factor = K.variable(self.beta, dtype=K.floatx(), name='beta_factor')\n",
        "        if self.trainable:\n",
        "            self._trainable_weights.append(self.beta_factor)\n",
        "\n",
        "        super(Snake, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        return snake_(inputs, self.beta_factor)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'beta': self.get_weights()[0] if self.trainable else self.beta, 'trainable': self.trainable}\n",
        "        base_config = super(Snake, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlwL_pGkEHSV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:01.544474Z",
          "iopub.execute_input": "2021-07-13T23:39:01.545122Z",
          "iopub.status.idle": "2021-07-13T23:39:02.567872Z",
          "shell.execute_reply.started": "2021-07-13T23:39:01.545082Z",
          "shell.execute_reply": "2021-07-13T23:39:02.566917Z"
        },
        "trusted": true,
        "id": "2RRJRSHjEHSV"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "\n",
        "from tensorflow import expand_dims\n",
        "from tensorflow.compat.v1 import image\n",
        "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, UpSampling2D, Conv2DTranspose, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Conv2D, DepthwiseConv2D, Lambda\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, concatenate, multiply, add\n",
        "from tensorflow.keras.layers import ReLU, LeakyReLU, PReLU, ELU, Softmax\n",
        "\n",
        "def decode_layer(X, channel, pool_size, unpool, kernel_size=3, \n",
        "                 activation='ReLU', batch_norm=False, name='decode'):\n",
        "    '''\n",
        "    An overall decode layer, based on either upsampling or trans conv.\n",
        "    \n",
        "    decode_layer(X, channel, pool_size, unpool, kernel_size=3,\n",
        "                 activation='ReLU', batch_norm=False, name='decode')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        pool_size: the decoding factor.\n",
        "        channel: (for trans conv only) number of convolution filters.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.           \n",
        "        kernel_size: size of convolution kernels. \n",
        "                     If kernel_size='auto', then it equals to the `pool_size`.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    * The defaut: `kernel_size=3`, is suitable for `pool_size=2`.\n",
        "    \n",
        "    '''\n",
        "    # parsers\n",
        "    if unpool is False:\n",
        "        # trans conv configurations\n",
        "        bias_flag = not batch_norm\n",
        "    \n",
        "    elif unpool == 'nearest':\n",
        "        # upsample2d configurations\n",
        "        unpool = True\n",
        "        interp = 'nearest'\n",
        "    \n",
        "    elif (unpool is True) or (unpool == 'bilinear'):\n",
        "        # upsample2d configurations\n",
        "        unpool = True\n",
        "        interp = 'bilinear'\n",
        "    \n",
        "    else:\n",
        "        raise ValueError('Invalid unpool keyword')\n",
        "        \n",
        "    if unpool:\n",
        "        X = UpSampling2D(size=(pool_size, pool_size), interpolation=interp, name='{}_unpool'.format(name))(X)\n",
        "    else:\n",
        "        if kernel_size == 'auto':\n",
        "            kernel_size = pool_size\n",
        "            \n",
        "        X = Conv2DTranspose(channel, kernel_size, strides=(pool_size, pool_size), \n",
        "                            padding='same', name='{}_trans_conv'.format(name))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\n",
        "            \n",
        "        # activation\n",
        "        if activation is not None:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "        X = Dropout(0.2)(X)\n",
        "    return X\n",
        "\n",
        "def encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n",
        "                 activation='ReLU', batch_norm=False, name='encode'):\n",
        "    '''\n",
        "    An overall encode layer, based on one of the:\n",
        "    (1) max-pooling, (2) average-pooling, (3) strided conv2d.\n",
        "    \n",
        "    encode_layer(X, channel, pool_size, pool, kernel_size='auto', \n",
        "                 activation='ReLU', batch_norm=False, name='encode')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        pool_size: the encoding factor.\n",
        "        channel: (for strided conv only) number of convolution filters.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        kernel_size: size of convolution kernels. \n",
        "                     If kernel_size='auto', then it equals to the `pool_size`.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    # parsers\n",
        "    if (pool in [False, True, 'max', 'ave']) is not True:\n",
        "        raise ValueError('Invalid pool keyword')\n",
        "        \n",
        "    # maxpooling2d as default\n",
        "    if pool is True:\n",
        "        pool = 'max'\n",
        "        \n",
        "    elif pool is False:\n",
        "        # stride conv configurations\n",
        "        bias_flag = not batch_norm\n",
        "    \n",
        "    if pool == 'max':\n",
        "        X = MaxPooling2D(pool_size=(pool_size, pool_size), name='{}_maxpool'.format(name))(X)\n",
        "        \n",
        "    elif pool == 'ave':\n",
        "        X = AveragePooling2D(pool_size=(pool_size, pool_size), name='{}_avepool'.format(name))(X)\n",
        "        \n",
        "    else:\n",
        "        if kernel_size == 'auto':\n",
        "            kernel_size = pool_size\n",
        "        \n",
        "        # linear convolution with strides\n",
        "        X = Conv2D(channel, kernel_size, strides=(pool_size, pool_size), \n",
        "                   padding='valid', use_bias=bias_flag, name='{}_stride_conv'.format(name))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_bn'.format(name))(X)\n",
        "            \n",
        "        # activation\n",
        "        if activation is not None:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "        X = Dropout(0.2)(X)\n",
        "    return X\n",
        "\n",
        "def attention_gate(X, g, channel,  \n",
        "                   activation='ReLU', \n",
        "                   attention='add', name='att'):\n",
        "    '''\n",
        "    Self-attention gate modified from Oktay et al. 2018.\n",
        "    \n",
        "    attention_gate(X, g, channel,  activation='ReLU', attention='add', name='att')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor, i.e., key and value.\n",
        "        g: gated tensor, i.e., query.\n",
        "        channel: number of intermediate channel.\n",
        "                 Oktay et al. (2018) did not specify (denoted as F_int).\n",
        "                 intermediate channel is expected to be smaller than the input channel.\n",
        "        activation: a nonlinear attnetion activation.\n",
        "                    The `sigma_1` in Oktay et al. 2018. Default is 'ReLU'.\n",
        "        attention: 'add' for additive attention; 'multiply' for multiplicative attention.\n",
        "                   Oktay et al. 2018 applied additive attention.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X_att: output tensor.\n",
        "    \n",
        "    '''\n",
        "    activation_func = eval(activation)\n",
        "    attention_func = eval(attention)\n",
        "    \n",
        "    # mapping the input tensor to the intermediate channel\n",
        "    theta_att = Conv2D(channel, 1, use_bias=True, name='{}_theta_x'.format(name))(X)\n",
        "    \n",
        "    # mapping the gate tensor\n",
        "    phi_g = Conv2D(channel, 1, use_bias=True, name='{}_phi_g'.format(name))(g)\n",
        "    \n",
        "    # ----- attention learning ----- #\n",
        "    query = attention_func([theta_att, phi_g], name='{}_add'.format(name))\n",
        "    \n",
        "    # nonlinear activation\n",
        "    f = activation_func(name='{}_activation'.format(name))(query)\n",
        "    \n",
        "    # linear transformation\n",
        "    psi_f = Conv2D(1, 1, use_bias=True, name='{}_psi_f'.format(name))(f)\n",
        "    # ------------------------------ #\n",
        "    \n",
        "    # sigmoid activation as attention coefficients\n",
        "    coef_att = Activation('sigmoid', name='{}_sigmoid'.format(name))(psi_f)\n",
        "    \n",
        "    # multiplicative attention masking\n",
        "    X_att = multiply([X, coef_att], name='{}_masking'.format(name))\n",
        "    \n",
        "    return X_att\n",
        "\n",
        "def CONV_stack(X, channel, kernel_size=3, stack_num=2, \n",
        "               dilation_rate=1, activation='ReLU', \n",
        "               batch_norm=False, name='conv_stack'):\n",
        "    '''\n",
        "    Stacked convolutional layers:\n",
        "    (Convolutional layer --> batch normalization --> Activation)*stack_num\n",
        "    \n",
        "    CONV_stack(X, channel, kernel_size=3, stack_num=2, dilation_rate=1, activation='ReLU', \n",
        "               batch_norm=False, name='conv_stack')\n",
        "    \n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked Conv2D-BN-Activation layers.\n",
        "        dilation_rate: optional dilated convolution kernel.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    bias_flag = not batch_norm\n",
        "    \n",
        "    # stacking Convolutional layers\n",
        "    for i in range(stack_num):\n",
        "        \n",
        "        activation_func = eval(activation)\n",
        "        \n",
        "        # linear convolution\n",
        "        X = Conv2D(channel, kernel_size, padding='same', use_bias=bias_flag, \n",
        "                   dilation_rate=dilation_rate, name='{}_{}'.format(name, i))(X)\n",
        "        \n",
        "        # batch normalization\n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(axis=3, name='{}_{}_bn'.format(name, i))(X)\n",
        "        \n",
        "        # activation\n",
        "        activation_func = eval(activation)\n",
        "        X = activation_func(name='{}_{}_activation'.format(name, i))(X)\n",
        "        \n",
        "    return X\n",
        "\n",
        "def Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv'):\n",
        "    '''\n",
        "    Stacked convolutional layers with residual path.\n",
        "     \n",
        "    Res_CONV_stack(X, X_skip, channel, res_num, activation='ReLU', batch_norm=False, name='res_conv')\n",
        "     \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        X_skip: the tensor that does go into the residual path \n",
        "                can be a copy of X (e.g., the identity block of ResNet).\n",
        "        channel: number of convolution filters.\n",
        "        res_num: number of convolutional layers within the residual path.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''  \n",
        "    X = CONV_stack(X, channel, kernel_size=3, stack_num=res_num, dilation_rate=1, \n",
        "                   activation=activation, batch_norm=batch_norm, name=name)\n",
        "\n",
        "    X = add([X_skip, X], name='{}_add'.format(name))\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "    X = activation_func(name='{}_add_activation'.format(name))(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv'):\n",
        "    '''\n",
        "    Depthwise separable convolution with (optional) dilated convolution kernel and batch normalization.\n",
        "    \n",
        "    Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, dilation_rate=1, activation='ReLU', batch_norm=False, name='sep_conv')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of stacked depthwise-pointwise layers.\n",
        "        dilation_rate: optional dilated convolution kernel.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "    bias_flag = not batch_norm\n",
        "    \n",
        "    for i in range(stack_num):\n",
        "        X = DepthwiseConv2D(kernel_size, dilation_rate=dilation_rate, padding='same', \n",
        "                            use_bias=bias_flag, name='{}_{}_depthwise'.format(name, i))(X)\n",
        "        \n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(name='{}_{}_depthwise_BN'.format(name, i))(X)\n",
        "\n",
        "        X = activation_func(name='{}_{}_depthwise_activation'.format(name, i))(X)\n",
        "\n",
        "        X = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_{}_pointwise'.format(name, i))(X)\n",
        "        \n",
        "        if batch_norm:\n",
        "            X = BatchNormalization(name='{}_{}_pointwise_BN'.format(name, i))(X)\n",
        "\n",
        "        X = activation_func(name='{}_{}_pointwise_activation'.format(name, i))(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp'):\n",
        "    '''\n",
        "    Atrous Spatial Pyramid Pooling (ASPP).\n",
        "    \n",
        "    ASPP_conv(X, channel, activation='ReLU', batch_norm=True, name='aspp')\n",
        "    \n",
        "    ----------\n",
        "    Wang, Y., Liang, B., Ding, M. and Li, J., 2019. Dense semantic labeling \n",
        "    with atrous spatial pyramid pooling and decoder for high-resolution remote \n",
        "    sensing imagery. Remote Sensing, 11(1), p.20.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., ReLU.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    * dilation rates are fixed to `[6, 9, 12]`.\n",
        "    '''\n",
        "    \n",
        "    activation_func = eval(activation)\n",
        "    bias_flag = not batch_norm\n",
        "\n",
        "    shape_before = X.get_shape().as_list()\n",
        "    b4 = GlobalAveragePooling2D(name='{}_avepool_b4'.format(name))(X)\n",
        "    \n",
        "    b4 = expand_dims(expand_dims(b4, 1), 1, name='{}_expdim_b4'.format(name))\n",
        "    \n",
        "    b4 = Conv2D(channel, 1, padding='same', use_bias=bias_flag, name='{}_conv_b4'.format(name))(b4)\n",
        "    \n",
        "    if batch_norm:\n",
        "        b4 = BatchNormalization(name='{}_conv_b4_BN'.format(name))(b4)\n",
        "        \n",
        "    b4 = activation_func(name='{}_conv_b4_activation'.format(name))(b4)\n",
        "    \n",
        "    # <----- tensorflow v1 resize.\n",
        "    b4 = Lambda(lambda X: image.resize(X, shape_before[1:3], method='bilinear', align_corners=True), \n",
        "                name='{}_resize_b4'.format(name))(b4)\n",
        "    \n",
        "    b0 = Conv2D(channel, (1, 1), padding='same', use_bias=bias_flag, name='{}_conv_b0'.format(name))(X)\n",
        "\n",
        "    if batch_norm:\n",
        "        b0 = BatchNormalization(name='{}_conv_b0_BN'.format(name))(b0)\n",
        "        \n",
        "    b0 = activation_func(name='{}_conv_b0_activation'.format(name))(b0)\n",
        "    \n",
        "    # dilation rates are fixed to `[6, 9, 12]`.\n",
        "    b_r6 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n",
        "                        dilation_rate=6, batch_norm=True, name='{}_sepconv_r6'.format(name))\n",
        "    b_r9 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n",
        "                        dilation_rate=9, batch_norm=True, name='{}_sepconv_r9'.format(name))\n",
        "    b_r12 = Sep_CONV_stack(X, channel, kernel_size=3, stack_num=1, activation='ReLU', \n",
        "                        dilation_rate=12, batch_norm=True, name='{}_sepconv_r12'.format(name))\n",
        "    \n",
        "    return concatenate([b4, b0, b_r6, b_r9, b_r12])\n",
        "\n",
        "def CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output'):\n",
        "    '''\n",
        "    Convolutional layer with output activation.\n",
        "    \n",
        "    CONV_output(X, n_labels, kernel_size=1, activation='Softmax', name='conv_output')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        n_labels: number of classification label(s).\n",
        "        kernel_size: size of 2-d convolution kernels. Default is 1-by-1.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                    Default option is 'Softmax'.\n",
        "                    if None is received, then linear activation is applied.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    X = Conv2D(n_labels, kernel_size, padding='same', use_bias=True, name=name)(X)\n",
        "    \n",
        "    if activation:\n",
        "        \n",
        "        if activation == 'Sigmoid':\n",
        "            X = Activation('sigmoid', name='{}_activation'.format(name))(X)\n",
        "            \n",
        "        else:\n",
        "            activation_func = eval(activation)\n",
        "            X = activation_func(name='{}_activation'.format(name))(X)\n",
        "            \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.571246Z",
          "iopub.execute_input": "2021-07-13T23:39:02.571620Z",
          "iopub.status.idle": "2021-07-13T23:39:02.583219Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.571581Z",
          "shell.execute_reply": "2021-07-13T23:39:02.582183Z"
        },
        "trusted": true,
        "id": "vo_W65bGEHSZ"
      },
      "source": [
        "def UNET_left(X, channel, kernel_size=3, stack_num=2, activation='ReLU', \n",
        "              pool=True, batch_norm=False, name='left0'):\n",
        "    '''\n",
        "    The encoder block of U-net.\n",
        "    \n",
        "    UNET_left(X, channel, kernel_size=3, stack_num=2, activation='ReLU', \n",
        "              pool=True, batch_norm=False, name='left0')\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of convolutional layers.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "        \n",
        "    '''\n",
        "    pool_size = 2\n",
        "    \n",
        "    X = encode_layer(X, channel, pool_size, pool, activation=activation, \n",
        "                     batch_norm=batch_norm, name='{}_encode'.format(name))\n",
        "\n",
        "    X = CONV_stack(X, channel, kernel_size, stack_num=stack_num, activation=activation, \n",
        "                   batch_norm=batch_norm, name='{}_conv'.format(name))\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "def UNET_right(X, X_list, channel, kernel_size=3, \n",
        "               stack_num=2, activation='ReLU',\n",
        "               unpool=True, batch_norm=False, concat=True, name='right0'):\n",
        "    \n",
        "    '''\n",
        "    The decoder block of U-net.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        X: input tensor.\n",
        "        X_list: a list of other tensors that connected to the input tensor.\n",
        "        channel: number of convolution filters.\n",
        "        kernel_size: size of 2-d convolution kernels.\n",
        "        stack_num: number of convolutional layers.\n",
        "        activation: one of the `tensorflow.keras.layers` interface, e.g., 'ReLU'.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.\n",
        "        batch_norm: True for batch normalization, False otherwise.\n",
        "        concat: True for concatenating the corresponded X_list elements.\n",
        "        name: prefix of the created keras layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        X: output tensor.\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    pool_size = 2\n",
        "    \n",
        "    X = decode_layer(X, channel, pool_size, unpool, \n",
        "                     activation=activation, batch_norm=batch_norm, name='{}_decode'.format(name))\n",
        "    \n",
        "    # linear convolutional layers before concatenation\n",
        "    X = CONV_stack(X, channel, kernel_size, stack_num=1, activation=activation, \n",
        "                   batch_norm=batch_norm, name='{}_conv_before_concat'.format(name))\n",
        "    if concat:\n",
        "        # <--- *stacked convolutional can be applied here\n",
        "        X = concatenate([X,]+X_list, axis=3, name=name+'_concat')\n",
        "    \n",
        "    # Stacked convolutions after concatenation \n",
        "    X = CONV_stack(X, channel, kernel_size, stack_num=stack_num, activation=activation, \n",
        "                   batch_norm=batch_norm, name=name+'_conv_after_concat')\n",
        "    \n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.585376Z",
          "iopub.execute_input": "2021-07-13T23:39:02.585968Z",
          "iopub.status.idle": "2021-07-13T23:39:02.601557Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.585930Z",
          "shell.execute_reply": "2021-07-13T23:39:02.600649Z"
        },
        "trusted": true,
        "id": "DZJdHaB1EHSc"
      },
      "source": [
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "layer_cadidates = {\n",
        "    'VGG16': ('block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3'),\n",
        "    'VGG19': ('block1_conv2', 'block2_conv2', 'block3_conv4', 'block4_conv4', 'block5_conv4'),\n",
        "    'ResNet50': ('conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block6_out', 'conv5_block3_out'),\n",
        "    'ResNet101': ('conv1_relu', 'conv2_block3_out', 'conv3_block4_out', 'conv4_block23_out', 'conv5_block3_out'),\n",
        "    'ResNet152': ('conv1_relu', 'conv2_block3_out', 'conv3_block8_out', 'conv4_block36_out', 'conv5_block3_out'),\n",
        "    'ResNet50V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block4_1_relu', 'conv4_block6_1_relu', 'post_relu'),\n",
        "    'ResNet101V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block4_1_relu', 'conv4_block23_1_relu', 'post_relu'),\n",
        "    'ResNet152V2': ('conv1_conv', 'conv2_block3_1_relu', 'conv3_block8_1_relu', 'conv4_block36_1_relu', 'post_relu'),\n",
        "    'DenseNet121': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n",
        "    'DenseNet169': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n",
        "    'DenseNet201': ('conv1/relu', 'pool2_conv', 'pool3_conv', 'pool4_conv', 'relu'),\n",
        "    'EfficientNetB0': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB1': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB2': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB3': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB4': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB5': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB6': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),\n",
        "    'EfficientNetB7': ('block2a_expand_activation', 'block3a_expand_activation', 'block4a_expand_activation', 'block6a_expand_activation', 'top_activation'),}\n",
        "\n",
        "def bach_norm_checker(backbone_name, batch_norm):\n",
        "    '''batch norm checker'''\n",
        "    if 'VGG' in backbone_name:\n",
        "        batch_norm_backbone = False\n",
        "    else:\n",
        "        batch_norm_backbone = True\n",
        "        \n",
        "    if batch_norm_backbone != batch_norm:       \n",
        "        if batch_norm_backbone:    \n",
        "            param_mismatch = \"\\n\\nBackbone {} uses batch norm, but other layers received batch_norm={}\".format(backbone_name, batch_norm)\n",
        "        else:\n",
        "            param_mismatch = \"\\n\\nBackbone {} does not use batch norm, but other layers received batch_norm={}\".format(backbone_name, batch_norm)\n",
        "            \n",
        "        warnings.warn(param_mismatch);\n",
        "        \n",
        "def backbone_zoo(backbone_name, weights, input_tensor, depth, freeze_backbone, freeze_batch_norm):\n",
        "    '''\n",
        "    Configuring a user specified encoder model based on the `tensorflow.keras.applications`\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        backbone_name: the bakcbone model name. Expected as one of the `tensorflow.keras.applications` class.\n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0,7]\n",
        "                       \n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        input_tensor: the input tensor \n",
        "        depth: number of encoded feature maps. \n",
        "               If four dwonsampling levels are needed, then depth=4.\n",
        "        \n",
        "        freeze_backbone: True for a frozen backbone\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.\n",
        "        \n",
        "    Output\n",
        "    ----------\n",
        "        model: a keras backbone model.\n",
        "        \n",
        "    '''\n",
        "    \n",
        "    cadidate = layer_cadidates[backbone_name]\n",
        "    \n",
        "    # ----- #\n",
        "    # depth checking\n",
        "    depth_max = len(cadidate)\n",
        "    if depth > depth_max:\n",
        "        depth = depth_max\n",
        "    # ----- #\n",
        "    \n",
        "    backbone_func = eval(backbone_name)\n",
        "    backbone_ = backbone_func(include_top=False, weights=weights, input_tensor=input_tensor, pooling=None,)\n",
        "    \n",
        "    X_skip = []\n",
        "    \n",
        "    for i in range(depth):\n",
        "        X_skip.append(backbone_.get_layer(cadidate[i]).output)\n",
        "        \n",
        "    model = Model(inputs=[input_tensor,], outputs=X_skip, name='{}_backbone'.format(backbone_name))\n",
        "    \n",
        "    if freeze_backbone:\n",
        "        \n",
        "        model = freeze_model(model, freeze_batch_norm=freeze_batch_norm)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.603018Z",
          "iopub.execute_input": "2021-07-13T23:39:02.603455Z",
          "iopub.status.idle": "2021-07-13T23:39:02.614056Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.603419Z",
          "shell.execute_reply": "2021-07-13T23:39:02.613199Z"
        },
        "trusted": true,
        "id": "_YBJweU1EHSd"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Conv2D, Dropout, Activation, UpSampling2D, GlobalMaxPooling2D, multiply\n",
        "from tensorflow.keras.backend import max\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.615573Z",
          "iopub.execute_input": "2021-07-13T23:39:02.617323Z",
          "iopub.status.idle": "2021-07-13T23:39:02.657353Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.617290Z",
          "shell.execute_reply": "2021-07-13T23:39:02.656434Z"
        },
        "trusted": true,
        "id": "foWjMJBBEHSd"
      },
      "source": [
        "### Unet 3+ Plus\n",
        "\n",
        "def unet_3plus_2d_base(input_tensor, filter_num_down, filter_num_skip, filter_num_aggregate, \n",
        "                       stack_num_down=2, stack_num_up=1, activation='ReLU', batch_norm=False, pool=True, unpool=True, \n",
        "                       backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus'):\n",
        "    '''\n",
        "    The base of UNET 3+ with an optional ImagNet-trained backbone.\n",
        "    \n",
        "    unet_3plus_2d_base(input_tensor, filter_num_down, filter_num_skip, filter_num_aggregate, \n",
        "                       stack_num_down=2, stack_num_up=1, activation='ReLU', batch_norm=False, pool=True, unpool=True, \n",
        "                       backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n",
        "                  \n",
        "    ----------\n",
        "    Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W. and Wu, J., 2020. \n",
        "    UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. \n",
        "    In ICASSP 2020-2020 IEEE International Conference on Acoustics, \n",
        "    Speech and Signal Processing (ICASSP) (pp. 1055-1059). IEEE.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_tensor: the input tensor of the base, e.g., `keras.layers.Inpyt((None, None, 3))`.        \n",
        "        filter_num_down: a list that defines the number of filters for each \n",
        "                         downsampling level. e.g., `[64, 128, 256, 512, 1024]`.\n",
        "                         the network depth is expected as `len(filter_num_down)`\n",
        "        filter_num_skip: a list that defines the number of filters after each \n",
        "                         full-scale skip connection. Number of elements is expected to be `depth-1`.\n",
        "                         i.e., the bottom level is not included.\n",
        "                         * Huang et al. (2020) applied the same numbers for all levels. \n",
        "                           e.g., `[64, 64, 64, 64]`.\n",
        "        filter_num_aggregate: an int that defines the number of channels of full-scale aggregations.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after full-scale concat) per upsampling level/block.          \n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., ReLU                \n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.     \n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.   \n",
        "    * Downsampling is achieved through maxpooling and can be replaced by strided convolutional layers here.\n",
        "    * Upsampling is achieved through bilinear interpolation and can be replaced by transpose convolutional layers here.\n",
        "    \n",
        "    Output\n",
        "    ----------\n",
        "        A list of tensors with the first/second/third tensor obtained from \n",
        "        the deepest/second deepest/third deepest upsampling block, etc.\n",
        "        * The feature map sizes of these tensors are different, \n",
        "          with the first tensor has the smallest size. \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    depth_ = len(filter_num_down)\n",
        "\n",
        "    X_encoder = []\n",
        "    X_decoder = []\n",
        "\n",
        "    # no backbone cases\n",
        "    if backbone is None:\n",
        "\n",
        "        X = input_tensor\n",
        "        X = ASPP_conv(X, filter_num_down[0], activation='ReLU', batch_norm=True , name='{}_ASPP_conv'.format(name))\n",
        "        # stacked conv2d before downsampling\n",
        "        #X = CONV_stack(X, filter_num_down[0], kernel_size=3, stack_num=stack_num_down, \n",
        "        #              activation=activation, batch_norm=batch_norm, name='{}_down0'.format(name))\n",
        "        X_encoder.append(X)\n",
        "\n",
        "        # downsampling levels\n",
        "        for i, f in enumerate(filter_num_down[1:]):\n",
        "\n",
        "            # UNET-like downsampling\n",
        "            X = UNET_left(X, f, kernel_size=3, stack_num=stack_num_down, activation=activation, \n",
        "                          pool=pool, batch_norm=batch_norm, name='{}_down{}'.format(name, i+1))\n",
        "            X_encoder.append(X)\n",
        "\n",
        "    else:\n",
        "        # handling VGG16 and VGG19 separately\n",
        "        if 'VGG' in backbone:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_encoder = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_encoder)\n",
        "\n",
        "        # for other backbones\n",
        "        else:\n",
        "            backbone_ = backbone_zoo(backbone, weights, input_tensor, depth_-1, freeze_backbone, freeze_batch_norm)\n",
        "            # collecting backbone feature maps\n",
        "            X_encoder = backbone_([input_tensor,])\n",
        "            depth_encode = len(X_encoder) + 1\n",
        "\n",
        "        # extra conv2d blocks are applied\n",
        "        # if downsampling levels of a backbone < user-specified downsampling levels\n",
        "        if depth_encode < depth_:\n",
        "\n",
        "            # begins at the deepest available tensor  \n",
        "            X = X_encoder[-1]\n",
        "\n",
        "            # extra downsamplings\n",
        "            for i in range(depth_-depth_encode):\n",
        "\n",
        "                i_real = i + depth_encode\n",
        "\n",
        "                X = UNET_left(X, filter_num_down[i_real], stack_num=stack_num_down, activation=activation, pool=pool, \n",
        "                              batch_norm=batch_norm, name='{}_down{}'.format(name, i_real+1))\n",
        "                X_encoder.append(X)\n",
        "\n",
        "\n",
        "    # treat the last encoded tensor as the first decoded tensor\n",
        "    X_decoder.append(X_encoder[-1])\n",
        "\n",
        "    # upsampling levels\n",
        "    X_encoder = X_encoder[::-1]\n",
        "\n",
        "    depth_decode = len(X_encoder)-1\n",
        "\n",
        "    # loop over upsampling levels\n",
        "    for i in range(depth_decode):\n",
        "\n",
        "        f = filter_num_skip[i]\n",
        "\n",
        "        # collecting tensors for layer fusion\n",
        "        X_fscale = []\n",
        "\n",
        "        # for each upsampling level, loop over all available downsampling levels (similar to the unet++)\n",
        "        for lev in range(depth_decode):\n",
        "\n",
        "            # counting scale difference between the current down- and upsampling levels\n",
        "            pool_scale = lev-i-1 # -1 for python indexing\n",
        "\n",
        "            # deeper tensors are obtained from **decoder** outputs\n",
        "            if pool_scale < 0:\n",
        "                pool_size = 2**(-1*pool_scale)\n",
        "                \n",
        "                X = decode_layer(X_decoder[lev], f, pool_size, unpool, \n",
        "                     activation=activation, batch_norm=batch_norm, name='{}_up_{}_en{}'.format(name, i, lev))\n",
        "\n",
        "            # unet skip connection (identity mapping)    \n",
        "            elif pool_scale == 0:\n",
        "\n",
        "                X = X_encoder[lev]\n",
        "\n",
        "            # shallower tensors are obtained from **encoder** outputs\n",
        "            else:\n",
        "                pool_size = 2**(pool_scale)\n",
        "                \n",
        "                X = encode_layer(X_encoder[lev], f, pool_size, pool, activation=activation, \n",
        "                                 batch_norm=batch_norm, name='{}_down_{}_en{}'.format(name, i, lev))\n",
        "\n",
        "            # a conv layer after feature map scale change\n",
        "            #X = CONV_stack(X, f, kernel_size=3, stack_num=1, \n",
        "            #               activation=activation, batch_norm=batch_norm, name='{}_down_from{}_to{}'.format(name, i, lev))\n",
        "            X = ASPP_conv(X, f , activation='ReLU', batch_norm=True , name='{}_down_from{}_to{}'.format(name, i, lev))\n",
        "            X_fscale.append(X)  \n",
        "\n",
        "        # layer fusion at the end of each level\n",
        "        # stacked conv layers after concat. BatchNormalization is fixed to True\n",
        "\n",
        "        X = concatenate(X_fscale, axis=-1, name='{}_concat_{}'.format(name, i))\n",
        "        X = CONV_stack(X, filter_num_aggregate, kernel_size=3, stack_num=stack_num_up, \n",
        "                       activation=activation, batch_norm=True, name='{}_fusion_conv_{}'.format(name, i))\n",
        "        X_decoder.append(X)\n",
        "\n",
        "    # if tensors for concatenation is not enough\n",
        "    # then use upsampling without concatenation \n",
        "    if depth_decode < depth_-1:\n",
        "        for i in range(depth_-depth_decode-1):\n",
        "            i_real = i + depth_decode\n",
        "            X = UNET_right(X, None, filter_num_aggregate, stack_num=stack_num_up, activation=activation, \n",
        "                           unpool=unpool, batch_norm=batch_norm, concat=False, name='{}_plain_up{}'.format(name, i_real))\n",
        "            X_decoder.append(X)\n",
        "        \n",
        "    # return decoder outputs\n",
        "    return X_decoder\n",
        "\n",
        "def unet_3plus_2d(input_size, n_labels, filter_num_down, filter_num_skip='auto', filter_num_aggregate='auto', \n",
        "                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='sigmoid',\n",
        "                  batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                  backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus'):\n",
        "    \n",
        "    '''\n",
        "    UNET 3+ with an optional ImageNet-trained backbone.\n",
        "    \n",
        "    unet_3plus_2d(input_size, n_labels, filter_num_down, filter_num_skip='auto', filter_num_aggregate='auto', \n",
        "                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n",
        "                  batch_norm=False, pool=True, unpool=True, deep_supervision=False, \n",
        "                  backbone=None, weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')\n",
        "                  \n",
        "    ----------\n",
        "    Huang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y., Han, X., Chen, Y.W. and Wu, J., 2020. \n",
        "    UNet 3+: A Full-Scale Connected UNet for Medical Image Segmentation. \n",
        "    In ICASSP 2020-2020 IEEE International Conference on Acoustics, \n",
        "    Speech and Signal Processing (ICASSP) (pp. 1055-1059). IEEE.\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        input_size: the size/shape of network input, e.g., `(128, 128, 3)`.\n",
        "        filter_num_down: a list that defines the number of filters for each \n",
        "                         downsampling level. e.g., `[64, 128, 256, 512, 1024]`.\n",
        "                         the network depth is expected as `len(filter_num_down)`\n",
        "        filter_num_skip: a list that defines the number of filters after each \n",
        "                         full-scale skip connection. Number of elements is expected to be `depth-1`.\n",
        "                         i.e., the bottom level is not included.\n",
        "                         * Huang et al. (2020) applied the same numbers for all levels. \n",
        "                           e.g., `[64, 64, 64, 64]`.\n",
        "        filter_num_aggregate: an int that defines the number of channels of full-scale aggregations.\n",
        "        stack_num_down: number of convolutional layers per downsampling level/block. \n",
        "        stack_num_up: number of convolutional layers (after full-scale concat) per upsampling level/block.\n",
        "        activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interfaces, e.g., 'ReLU'\n",
        "        output_activation: one of the `tensorflow.keras.layers` or `keras_unet_collection.activations` interface or 'Sigmoid'.\n",
        "                           Default option is 'Softmax'.\n",
        "                           if None is received, then linear activation is applied.\n",
        "        batch_norm: True for batch normalization.\n",
        "        pool: True or 'max' for MaxPooling2D.\n",
        "              'ave' for AveragePooling2D.\n",
        "              False for strided conv + batch norm + activation.\n",
        "        unpool: True or 'bilinear' for Upsampling2D with bilinear interpolation.\n",
        "                'nearest' for Upsampling2D with nearest interpolation.\n",
        "                False for Conv2DTranspose + batch norm + activation.   \n",
        "        deep_supervision: True for a model that supports deep supervision. Details see Huang et al. (2020).\n",
        "        name: prefix of the created keras model and its layers.\n",
        "        \n",
        "        ---------- (keywords of backbone options) ----------\n",
        "        backbone_name: the bakcbone model name. Should be one of the `tensorflow.keras.applications` class.\n",
        "                       None (default) means no backbone. \n",
        "                       Currently supported backbones are:\n",
        "                       (1) VGG16, VGG19\n",
        "                       (2) ResNet50, ResNet101, ResNet152\n",
        "                       (3) ResNet50V2, ResNet101V2, ResNet152V2\n",
        "                       (4) DenseNet121, DenseNet169, DenseNet201\n",
        "                       (5) EfficientNetB[0-7]\n",
        "        weights: one of None (random initialization), 'imagenet' (pre-training on ImageNet), \n",
        "                 or the path to the weights file to be loaded.\n",
        "        freeze_backbone: True for a frozen backbone.\n",
        "        freeze_batch_norm: False for not freezing batch normalization layers.   \n",
        "        \n",
        "    * The Classification-guided Module (CGM) is not implemented. \n",
        "      See https://github.com/yingkaisha/keras-unet-collection/tree/main/examples for a relevant example.\n",
        "    * Automated mode is applied for determining `filter_num_skip`, `filter_num_aggregate`.\n",
        "    * The default output activation is sigmoid, consistent with Huang et al. (2020).\n",
        "    * Downsampling is achieved through maxpooling and can be replaced by strided convolutional layers here.\n",
        "    * Upsampling is achieved through bilinear interpolation and can be replaced by transpose convolutional layers here.\n",
        "    \n",
        "    Output\n",
        "    ----------\n",
        "        model: a keras model.\n",
        "    \n",
        "    '''\n",
        "\n",
        "    depth_ = len(filter_num_down)\n",
        "    \n",
        "    verbose = False\n",
        "    \n",
        "    if filter_num_skip == 'auto':\n",
        "        verbose = True\n",
        "        filter_num_skip = [filter_num_down[0] for num in range(depth_-1)]\n",
        "        \n",
        "    if filter_num_aggregate == 'auto':\n",
        "        verbose = True\n",
        "        filter_num_aggregate = int(depth_*filter_num_down[0])\n",
        "        \n",
        "    if verbose:\n",
        "        print('Automated hyper-parameter determination is applied with the following details:\\n----------')\n",
        "        print('\\tNumber of convolution filters after each full-scale skip connection: filter_num_skip = {}'.format(filter_num_skip))\n",
        "        print('\\tNumber of channels of full-scale aggregated feature maps: filter_num_aggregate = {}'.format(filter_num_aggregate))    \n",
        "    \n",
        "    if backbone is not None:\n",
        "        bach_norm_checker(backbone, batch_norm)\n",
        "    \n",
        "    X_encoder = []\n",
        "    X_decoder = []\n",
        "\n",
        "\n",
        "    IN = Input(input_size)\n",
        "\n",
        "    X_decoder = unet_3plus_2d_base(IN, filter_num_down, filter_num_skip, filter_num_aggregate, \n",
        "                                   stack_num_down=stack_num_down, stack_num_up=stack_num_up, activation=activation, \n",
        "                                   batch_norm=batch_norm, pool=pool, unpool=unpool, \n",
        "                                   backbone=backbone, weights=weights, freeze_backbone=freeze_backbone, \n",
        "                                   freeze_batch_norm=freeze_batch_norm, name=name)\n",
        "    X_decoder = X_decoder[::-1]\n",
        "    \n",
        "    if deep_supervision:\n",
        "        \n",
        "        # ----- frozen backbone issue checker ----- #\n",
        "        if ('{}_backbone_'.format(backbone) in X_decoder[0].name) and freeze_backbone:\n",
        "            \n",
        "            backbone_warn = '\\n\\nThe deepest UNET 3+ deep supervision branch directly connects to a frozen backbone.\\nTesting your configurations on `keras_unet_collection.base.unet_plus_2d_base` is recommended.'\n",
        "            warnings.warn(backbone_warn);\n",
        "        # ----------------------------------------- #\n",
        "        \n",
        "        OUT_stack = []\n",
        "        L_out = len(X_decoder)\n",
        "        \n",
        "        print('----------\\ndeep_supervision = True\\nnames of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\\n\"final\" is the final output layer):\\n')\n",
        "        \n",
        "        # conv2d --> upsampling --> output activation.\n",
        "        # index 0 is final output \n",
        "        for i in range(1, L_out):\n",
        "            \n",
        "            pool_size = 2**(i)\n",
        "            \n",
        "            X = Conv2D(n_labels, 3, padding='same', name='{}_output_conv_{}'.format(name, i-1))(X_decoder[i])\n",
        "            \n",
        "            X = decode_layer(X, n_labels, pool_size, unpool, \n",
        "                             activation=None, batch_norm=False, name='{}_output_sup{}'.format(name, i-1))\n",
        "            \n",
        "            if output_activation:\n",
        "                print('\\t{}_output_sup{}_activation'.format(name, i-1))\n",
        "                \n",
        "                if output_activation == 'Sigmoid':\n",
        "                    X = Activation('sigmoid', name='{}_output_sup{}_activation'.format(name, i-1))(X)\n",
        "                else:\n",
        "                    activation_func = eval(output_activation)\n",
        "                    X = activation_func(name='{}_output_sup{}_activation'.format(name, i-1))(X)\n",
        "            else:\n",
        "                if unpool is False:\n",
        "                    print('\\t{}_output_sup{}_trans_conv'.format(name, i-1))\n",
        "                else:\n",
        "                    print('\\t{}_output_sup{}_unpool'.format(name, i-1))\n",
        "                    \n",
        "            OUT_stack.append(X)\n",
        "        \n",
        "        X = CONV_output(X_decoder[0], n_labels, kernel_size=3, \n",
        "                        activation=output_activation, name='{}_output_final'.format(name))\n",
        "        OUT_stack.append(X)\n",
        "        \n",
        "        \n",
        "        concat_out = concatenate([OUT_stack[0],OUT_stack[1] , OUT_stack[2] , OUT_stack[4]], axis=-1)\n",
        "        \n",
        "        X = CONV_output(concat_out, n_labels, kernel_size=3, \n",
        "                        activation=output_activation, name='{}concate_output1'.format(name))\n",
        "        OUT_stack.append(X)\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        if output_activation:\n",
        "            print('\\t{}_output_final_activation'.format(name))\n",
        "        else:\n",
        "            print('\\t{}_output_final'.format(name))\n",
        "            \n",
        "        model = Model([IN,], OUT_stack)\n",
        "\n",
        "    else:\n",
        "        OUT = CONV_output(X_decoder[0], n_labels, kernel_size=3, \n",
        "                          activation=output_activation, name='{}_output_final'.format(name))\n",
        "\n",
        "        model = Model([IN,], [OUT,])\n",
        "        \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-10T20:35:26.238587Z",
          "iopub.execute_input": "2021-07-10T20:35:26.238981Z",
          "iopub.status.idle": "2021-07-10T20:35:26.989595Z",
          "shell.execute_reply.started": "2021-07-10T20:35:26.238945Z",
          "shell.execute_reply": "2021-07-10T20:35:26.988775Z"
        },
        "id": "LdTdqJGgEHSk"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.771798Z",
          "iopub.execute_input": "2021-07-13T23:39:02.772217Z",
          "iopub.status.idle": "2021-07-13T23:39:02.905418Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.772180Z",
          "shell.execute_reply": "2021-07-13T23:39:02.904471Z"
        },
        "trusted": true,
        "id": "cziMbaV5EHSl",
        "outputId": "06ef01c8-a36f-4925-bf12-1503d3a44624"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage import io # To read a single image\n",
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import cv2\n",
        "datagen = ImageDataGenerator( #Randomly rotates but with atmost rotation as 45 degree , Randomly width is varied but with atmost 20% width change \n",
        "                        rotation_range = 45 , \n",
        "                        width_shift_range = 0.2,\n",
        "                        height_shift_range = 0.2 ,\n",
        "                        shear_range        = 0.2 , \n",
        "                        horizontal_flip = True , \n",
        "                        fill_mode = \"constant\" , cval = 125)\n",
        "\n",
        "def load_data(paths , split = 0.15):\n",
        "  images = []\n",
        "  masks = []\n",
        "  numImages = 0\n",
        "  numMasks = 0\n",
        "\n",
        "  for path in paths:\n",
        "    \n",
        "        \n",
        "    images.extend(sorted(glob(os.path.join(path , \"images/*\"))))\n",
        "    masks.extend(sorted(glob(os.path.join(path , \"label/*\"))))\n",
        "    if \"training\" in path:\n",
        "      #for i in [\"00\", \"01\" , \"02\" , \"03\" , \"04\" , \"05\" , \"07\", \"08\" , \"09\" , \"10\" , \"11\" , \"12\" , \"13\" , \"14\" , \"16\" , \"17\" , \"18\" , \"19\" , \"21\" , \"22\" , \"24\" , \"26\" , \"27\" ]:\n",
        "      #  images.remove(\"../input/chagas/ChagasTraining/training-20210525T143718Z-001/training/images/i8{}.xml\".format(i))\n",
        "\n",
        "      n = \"training\"\n",
        "    if \"Test\" in path:\n",
        "      n= \"Test\"\n",
        "    if \"Val\" in path:\n",
        "\n",
        "      n= \"Validation\"\n",
        "\n",
        "    print(\"{}PathImagesAre:\".format(n),len(images) - numImages)\n",
        "    print(\"{}PathImagesAre:\".format(n),len(masks) - numMasks)\n",
        "\n",
        "    numImages = len(images)\n",
        "    numMasks = len(masks)\n",
        "\n",
        "  total_size = len(images)\n",
        "  valid_size = int(split * total_size)\n",
        "  test_size = int(split * total_size)\n",
        "  print(\"Number of images in Total , Validation images and Test Images : \" , total_size , valid_size ,test_size  )\n",
        "\n",
        "  train_x , test_x = train_test_split(images , test_size = test_size , random_state = 42 )\n",
        "  train_y , test_y = train_test_split(masks , test_size = test_size , random_state = 42 )\n",
        "\n",
        "  train_x , valid_x = train_test_split(train_x , test_size = valid_size , random_state = 42 )\n",
        "  train_y , valid_y = train_test_split(train_y , test_size = valid_size , random_state = 42 )\n",
        "\n",
        "  return (train_x , train_y) , (valid_x , valid_y ) , (test_x , test_y) \n",
        "\n",
        "def read_image(path):\n",
        "  path = path.decode()\n",
        "  x = cv2.imread(path , cv2.IMREAD_COLOR)\n",
        "  #x = cv2.resize(x , (256 , 256))\n",
        "  x = x/255.0\n",
        "  #Size is 256 * 256 * 3\n",
        "\n",
        "  return x\n",
        "\n",
        "def read_mask(path):\n",
        "  path = path.decode()\n",
        "  x = cv2.imread(path , cv2.IMREAD_GRAYSCALE)\n",
        "  #x = cv2.resize(x , (256 , 256))\n",
        "  x = x/255.0\n",
        "  \n",
        "  #Size is 256 * 256\n",
        "  x = np.expand_dims(x , axis = -1)\n",
        "  #Size becames 256 * 256 * 1\n",
        "\n",
        "  return x\n",
        "\n",
        "def tf_parse(imagepath , maskpath):\n",
        "  def _parse(imagepath , maskpath):\n",
        "    x = read_image(imagepath)\n",
        "    y = read_mask(maskpath)\n",
        "\n",
        "    return x , y\n",
        "\n",
        "  x , y = tf.numpy_function(_parse , [imagepath , maskpath] , [tf.float64 , tf.float64] )\n",
        "  x.set_shape([512 , 512 , 3])\n",
        "  y.set_shape([512, 512, 1])\n",
        "\n",
        "  return x , y \n",
        "\n",
        "\n",
        "def tf_dataset( imagepath , maskpath , batch = 2):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((imagepath , maskpath))\n",
        "  dataset = dataset.map(tf_parse)\n",
        "  dataset = dataset.batch(batch)\n",
        "  dataset = dataset.repeat()\n",
        "  return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  paths = [\"../input/training/training\" , \"../input/testdata/Test\", \"../input/validation/Validation\"] \n",
        "  (train_x , train_y) , (valid_x , valid_y ) , (test_x , test_y)  =   load_data(paths)\n",
        "\n",
        "  print(\"Number of images in Train , Validation images and Test Images : \" ,len(train_x) , len(valid_x ) , len(test_x) )\n",
        "\n",
        "  ds = tf_dataset(test_x , test_y)\n",
        "  for x, y in ds:\n",
        "    \n",
        "    print(x.shape , y.shape)\n",
        "    break\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "trainingPathImagesAre: 600\ntrainingPathImagesAre: 600\nTestPathImagesAre: 200\nTestPathImagesAre: 200\nValidationPathImagesAre: 200\nValidationPathImagesAre: 200\nNumber of images in Total , Validation images and Test Images :  1000 150 150\nNumber of images in Train , Validation images and Test Images :  700 150 150\n(2, 512, 512, 3) (2, 512, 512, 1)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.906775Z",
          "iopub.execute_input": "2021-07-13T23:39:02.907067Z",
          "iopub.status.idle": "2021-07-13T23:39:02.913920Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.907042Z",
          "shell.execute_reply": "2021-07-13T23:39:02.913153Z"
        },
        "trusted": true,
        "id": "CVDjfAdpEHSn"
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.916219Z",
          "iopub.execute_input": "2021-07-13T23:39:02.916525Z",
          "iopub.status.idle": "2021-07-13T23:39:02.926225Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.916495Z",
          "shell.execute_reply": "2021-07-13T23:39:02.925424Z"
        },
        "trusted": true,
        "id": "lJh0Tr5KEHSo"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n",
        "from tensorflow.keras.metrics import Recall , Precision \n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:02.928134Z",
          "iopub.execute_input": "2021-07-13T23:39:02.928379Z",
          "iopub.status.idle": "2021-07-13T23:39:08.922701Z",
          "shell.execute_reply.started": "2021-07-13T23:39:02.928356Z",
          "shell.execute_reply": "2021-07-13T23:39:08.921698Z"
        },
        "trusted": true,
        "id": "r9-HU5K0EHSo",
        "outputId": "66a1fbc0-d1dc-48cd-e91f-736825917b67"
      },
      "source": [
        "pip install segmentation_models\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: segmentation_models in /opt/conda/lib/python3.7/site-packages (1.0.1)\nRequirement already satisfied: image-classifiers==1.0.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\nRequirement already satisfied: efficientnet==1.0.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.0)\nRequirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from segmentation_models) (1.0.8)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.0.0->segmentation_models) (0.18.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (2.10.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.19.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->segmentation_models) (1.15.0)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.1.1)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (7.2.0)\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (3.4.1)\nRequirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (1.5.4)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2021.4.8)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.5)\nRequirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.0.0->segmentation_models) (2.9.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.8.1)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0->segmentation_models) (0.10.0)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0->segmentation_models) (4.4.2)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:08.926073Z",
          "iopub.execute_input": "2021-07-13T23:39:08.926382Z",
          "iopub.status.idle": "2021-07-13T23:39:08.936250Z",
          "shell.execute_reply.started": "2021-07-13T23:39:08.926352Z",
          "shell.execute_reply": "2021-07-13T23:39:08.935390Z"
        },
        "trusted": true,
        "id": "hUAfpcWhEHSp",
        "outputId": "99d6a14e-0386-4903-c7bd-5f8c7d669a03"
      },
      "source": [
        "%env SM_FRAMEWORK=tf.keras\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "env: SM_FRAMEWORK=tf.keras\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:08.938097Z",
          "iopub.execute_input": "2021-07-13T23:39:08.938519Z",
          "iopub.status.idle": "2021-07-13T23:39:08.957635Z",
          "shell.execute_reply.started": "2021-07-13T23:39:08.938478Z",
          "shell.execute_reply": "2021-07-13T23:39:08.956523Z"
        },
        "trusted": true,
        "id": "qI-ZinyPEHSp"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1e-15\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "def iou_coeff(y_true, y_pred):\n",
        "    smooth=1e-15\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n",
        "    mvalue=(intersection+smooth)/(union+smooth)\n",
        "    return mvalue\n",
        "def iou_coeff_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "    Only computes a batch-wise average of precision.\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "        Only computes a batch-wise average of recall.\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "def ACL5(y_true, y_pred): \n",
        "\n",
        "    #y_pred = K.cast(y_pred, dtype = 'float64')\n",
        "\n",
        "    print(K.int_shape(y_pred))\n",
        "\n",
        "    x = y_pred[:,1:,:,:] - y_pred[:,:-1,:,:] # horizontal and vertical directions \n",
        "    y = y_pred[:,:,1:,:] - y_pred[:,:,:-1,:]\n",
        "\n",
        "    delta_x = x[:,1:,:-2,:]**2\n",
        "    delta_y = y[:,:-2,1:,:]**2\n",
        "    delta_u = K.abs(delta_x + delta_y) \n",
        "\n",
        "    epsilon = 0.00000001 # where is a parameter to avoid square root is zero in practice.\n",
        "    w = 1####\n",
        "    lenth = w * K.sum(K.sqrt(delta_u + epsilon)) # equ.(11) in the paper\n",
        "\n",
        "\n",
        "    C_1 = tf.ones((512, 512))\n",
        "    C_2 = tf.zeros((512, 512))\n",
        "\n",
        "    region_in = K.abs(K.sum( y_pred[:,:,:,0] * ((y_true[:,:,:,0] - C_1)**2) ) ) # equ.(12) in the paper\n",
        "    region_out = K.abs(K.sum( (1-y_pred[:,:,:,0]) * ((y_true[:,:,:,0] - C_2)**2) )) # equ.(12) in the paper\n",
        "\n",
        "    lambdaP = 5 # lambda parameter could be various.\n",
        "    \n",
        "    loss =  lenth + lambdaP * ((region_in) + (region_out)) \n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:08.959327Z",
          "iopub.execute_input": "2021-07-13T23:39:08.959976Z",
          "iopub.status.idle": "2021-07-13T23:39:17.553847Z",
          "shell.execute_reply.started": "2021-07-13T23:39:08.959920Z",
          "shell.execute_reply": "2021-07-13T23:39:17.553021Z"
        },
        "trusted": true,
        "id": "huj17MCBEHSq",
        "outputId": "8880ff71-66ae-477e-bb31-2f6de7d6f036"
      },
      "source": [
        "\n",
        "def freeze_model(model, freeze_batch_norm=False):\n",
        "    '''\n",
        "    freeze a keras model\n",
        "    \n",
        "    Input\n",
        "    ----------\n",
        "        model: a keras model\n",
        "        freeze_batch_norm: False for not freezing batch notmalization layers\n",
        "    '''\n",
        "    if freeze_batch_norm:\n",
        "        for layer in model.layers:\n",
        "            layer.trainable = False\n",
        "    else:\n",
        "        from tensorflow.keras.layers import BatchNormalization    \n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, BatchNormalization):\n",
        "                layer.trainable = True\n",
        "            else:\n",
        "                layer.trainable = False\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \n",
        "    name = 'unet3plus'\n",
        "    activation = 'ReLU'\n",
        "    filter_num_down = [32, 64, 128, 256, 512 , 1024]\n",
        "    filter_num_skip = [64, 64, 64, 64 , 64]\n",
        "    filter_num_aggregate = 160\n",
        "\n",
        "    stack_num_down = 2\n",
        "    stack_num_up = 1\n",
        "    n_labels = 1\n",
        "\n",
        "    # `unet_3plus_2d_base` accepts an input tensor \n",
        "    # and produces output tensors from different upsampling levels\n",
        "    # ---------------------------------------- #\n",
        "    #input_tensor = keras.layers.Input((512, 512, 3))\n",
        "    # base architecture\n",
        "\n",
        "    input_size = (512, 512, 3)\n",
        "    model = unet_3plus_2d(input_size, n_labels, filter_num_down, filter_num_skip=filter_num_skip, filter_num_aggregate=filter_num_aggregate, \n",
        "                  stack_num_down=2, stack_num_up=1, activation='ReLU', output_activation='Sigmoid',\n",
        "                  batch_norm=True, pool=True, unpool=False, deep_supervision=True, \n",
        "                  backbone= \"DenseNet201\", weights='imagenet', freeze_backbone=True, freeze_batch_norm=True, name='unet3plus')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "----------\ndeep_supervision = True\nnames of output tensors are listed as follows (\"sup0\" is the shallowest supervision layer;\n\"final\" is the final output layer):\n\n\tunet3plus_output_sup0_activation\n\tunet3plus_output_sup1_activation\n\tunet3plus_output_sup2_activation\n\tunet3plus_output_sup3_activation\n\tunet3plus_output_sup4_activation\n\tunet3plus_output_final_activation\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:39:17.556648Z",
          "iopub.execute_input": "2021-07-13T23:39:17.556921Z",
          "iopub.status.idle": "2021-07-13T23:53:28.292715Z",
          "shell.execute_reply.started": "2021-07-13T23:39:17.556894Z",
          "shell.execute_reply": "2021-07-13T23:53:28.291913Z"
        },
        "trusted": true,
        "id": "3DW_9GVTEHSq",
        "outputId": "276ebb3a-4c52-4013-b9bd-33a9897b20f4"
      },
      "source": [
        "import segmentation_models\n",
        "def iou_coeff_loss(y_true, y_pred):\n",
        "    \n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    union=K.sum(y_true_f) + K.sum(y_pred_f)-intersection\n",
        "    mvalue=(intersection+1e-6)/(union+1e-6)\n",
        "    mvalue = 1-mvalue\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from segmentation_models.losses import bce_jaccard_loss\n",
        "    from segmentation_models.metrics import iou_score\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "    #HYPERPARAMETER\n",
        "\n",
        "\n",
        "    batch = 2\n",
        "    lr = 1.00E-04\n",
        "    epochs = 2\n",
        "    opt = tf.keras.optimizers.Adam(lr)\n",
        "    metrics = [\"acc\" , Recall() , Precision() , iou]\n",
        "    #Data\n",
        "    train_dataset = tf_dataset(train_x, train_y, batch=batch)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch)\n",
        "    img_w, img_h, n_label = 512 , 512 , 1\n",
        "    #model = att_r2_unet(img_w, img_h, n_label, data_format='channels_last')\n",
        "    #final_output , model = build_model((512 , 512 , 3))\n",
        "    #model.summary()\n",
        "    #model.compile(loss=bce_jaccard_loss, optimizer=opt, metrics=metrics)\n",
        "    model.compile(optimizer= opt, loss=bce_jaccard_loss, metrics=[dice_loss,iou_coeff,precision,recall])\n",
        "    \n",
        "    callbacks = [ \n",
        "        ModelCheckpoint(\"files/model.h5\"),\n",
        "        ReduceLROnPlateau(monitor='iou', factor=0.1, patience=4),\n",
        "        #CSVLogger(\"/content/drive/MyDrive/data.csv\"),\n",
        "        TensorBoard(),\n",
        "        EarlyStopping(monitor='iou', patience=15, restore_best_weights=False)\n",
        "    ]\n",
        "    #Number Of Batches\n",
        "    train_steps = len(train_x)//batch\n",
        "    valid_steps = len(valid_x)//batch\n",
        "\n",
        "    if len(train_x) % batch != 0:\n",
        "        train_steps += 1\n",
        "    if len(valid_x) % batch != 0:\n",
        "        valid_steps += 1\n",
        "\n",
        "    model.fit(train_dataset,\n",
        "        validation_data=valid_dataset,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_steps,\n",
        "        callbacks=callbacks,\n",
        "        shuffle = False)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/2\n350/350 [==============================] - 448s 1s/step - loss: 10.5434 - unet3plus_output_sup0_activation_loss: 1.6213 - unet3plus_output_sup1_activation_loss: 1.6004 - unet3plus_output_sup2_activation_loss: 1.6721 - unet3plus_output_sup3_activation_loss: 1.6753 - unet3plus_output_sup4_activation_loss: 1.6759 - unet3plus_output_final_activation_loss: 0.8845 - unet3plusconcate_output1_activation_loss: 1.4140 - unet3plus_output_sup0_activation_dice_loss: 0.9711 - unet3plus_output_sup0_activation_iou_coeff: 0.0147 - unet3plus_output_sup0_activation_precision: 0.0187 - unet3plus_output_sup0_activation_recall: 0.3702 - unet3plus_output_sup1_activation_dice_loss: 0.9699 - unet3plus_output_sup1_activation_iou_coeff: 0.0153 - unet3plus_output_sup1_activation_precision: 0.0279 - unet3plus_output_sup1_activation_recall: 0.2375 - unet3plus_output_sup2_activation_dice_loss: 0.9740 - unet3plus_output_sup2_activation_iou_coeff: 0.0132 - unet3plus_output_sup2_activation_precision: 0.0104 - unet3plus_output_sup2_activation_recall: 0.0443 - unet3plus_output_sup3_activation_dice_loss: 0.9742 - unet3plus_output_sup3_activation_iou_coeff: 0.0131 - unet3plus_output_sup3_activation_precision: 0.0117 - unet3plus_output_sup3_activation_recall: 0.0061 - unet3plus_output_sup4_activation_dice_loss: 0.9742 - unet3plus_output_sup4_activation_iou_coeff: 0.0131 - unet3plus_output_sup4_activation_precision: 0.0113 - unet3plus_output_sup4_activation_recall: 0.0028 - unet3plus_output_final_activation_dice_loss: 0.6155 - unet3plus_output_final_activation_iou_coeff: 0.2713 - unet3plus_output_final_activation_precision: 0.4637 - unet3plus_output_final_activation_recall: 0.6600 - unet3plusconcate_output1_activation_dice_loss: 0.9765 - unet3plusconcate_output1_activation_iou_coeff: 0.0119 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 10.4966 - val_unet3plus_output_sup0_activation_loss: 1.5371 - val_unet3plus_output_sup1_activation_loss: 1.5345 - val_unet3plus_output_sup2_activation_loss: 1.6541 - val_unet3plus_output_sup3_activation_loss: 1.6613 - val_unet3plus_output_sup4_activation_loss: 1.6634 - val_unet3plus_output_final_activation_loss: 1.1211 - val_unet3plusconcate_output1_activation_loss: 1.3252 - val_unet3plus_output_sup0_activation_dice_loss: 0.9742 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup0_activation_precision: 0.0131 - val_unet3plus_output_sup0_activation_recall: 0.2420 - val_unet3plus_output_sup1_activation_dice_loss: 0.9737 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0134 - val_unet3plus_output_sup1_activation_precision: 0.0128 - val_unet3plus_output_sup1_activation_recall: 0.1164 - val_unet3plus_output_sup2_activation_dice_loss: 0.9739 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup2_activation_precision: 0.0105 - val_unet3plus_output_sup2_activation_recall: 0.0356 - val_unet3plus_output_sup3_activation_dice_loss: 0.9740 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup3_activation_precision: 0.0124 - val_unet3plus_output_sup3_activation_recall: 0.0037 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0096 - val_unet3plus_output_sup4_activation_recall: 0.0025 - val_unet3plus_output_final_activation_dice_loss: 0.9997 - val_unet3plus_output_final_activation_iou_coeff: 1.3662e-04 - val_unet3plus_output_final_activation_precision: 0.0000e+00 - val_unet3plus_output_final_activation_recall: 0.0000e+00 - val_unet3plusconcate_output1_activation_dice_loss: 0.9748 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0127 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 2/2\n350/350 [==============================] - 396s 1s/step - loss: 9.7695 - unet3plus_output_sup0_activation_loss: 1.5350 - unet3plus_output_sup1_activation_loss: 1.5398 - unet3plus_output_sup2_activation_loss: 1.6506 - unet3plus_output_sup3_activation_loss: 1.6571 - unet3plus_output_sup4_activation_loss: 1.6595 - unet3plus_output_final_activation_loss: 0.4259 - unet3plusconcate_output1_activation_loss: 1.3016 - unet3plus_output_sup0_activation_dice_loss: 0.9643 - unet3plus_output_sup0_activation_iou_coeff: 0.0182 - unet3plus_output_sup0_activation_precision: 0.0292 - unet3plus_output_sup0_activation_recall: 0.4513 - unet3plus_output_sup1_activation_dice_loss: 0.9657 - unet3plus_output_sup1_activation_iou_coeff: 0.0175 - unet3plus_output_sup1_activation_precision: 0.0341 - unet3plus_output_sup1_activation_recall: 0.2626 - unet3plus_output_sup2_activation_dice_loss: 0.9737 - unet3plus_output_sup2_activation_iou_coeff: 0.0133 - unet3plus_output_sup2_activation_precision: 0.0134 - unet3plus_output_sup2_activation_recall: 0.0360 - unet3plus_output_sup3_activation_dice_loss: 0.9741 - unet3plus_output_sup3_activation_iou_coeff: 0.0131 - unet3plus_output_sup3_activation_precision: 0.0202 - unet3plus_output_sup3_activation_recall: 0.0050 - unet3plus_output_sup4_activation_dice_loss: 0.9742 - unet3plus_output_sup4_activation_iou_coeff: 0.0131 - unet3plus_output_sup4_activation_precision: 0.0098 - unet3plus_output_sup4_activation_recall: 0.0021 - unet3plus_output_final_activation_dice_loss: 0.2510 - unet3plus_output_final_activation_iou_coeff: 0.6063 - unet3plus_output_final_activation_precision: 0.7864 - unet3plus_output_final_activation_recall: 0.7619 - unet3plusconcate_output1_activation_dice_loss: 0.9806 - unet3plusconcate_output1_activation_iou_coeff: 0.0098 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 9.6181 - val_unet3plus_output_sup0_activation_loss: 1.4592 - val_unet3plus_output_sup1_activation_loss: 1.4838 - val_unet3plus_output_sup2_activation_loss: 1.6317 - val_unet3plus_output_sup3_activation_loss: 1.6426 - val_unet3plus_output_sup4_activation_loss: 1.6473 - val_unet3plus_output_final_activation_loss: 0.4881 - val_unet3plusconcate_output1_activation_loss: 1.2653 - val_unet3plus_output_sup0_activation_dice_loss: 0.9578 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0216 - val_unet3plus_output_sup0_activation_precision: 0.0284 - val_unet3plus_output_sup0_activation_recall: 0.5386 - val_unet3plus_output_sup1_activation_dice_loss: 0.9627 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0190 - val_unet3plus_output_sup1_activation_precision: 0.0315 - val_unet3plus_output_sup1_activation_recall: 0.2980 - val_unet3plus_output_sup2_activation_dice_loss: 0.9734 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0135 - val_unet3plus_output_sup2_activation_precision: 0.0168 - val_unet3plus_output_sup2_activation_recall: 0.0391 - val_unet3plus_output_sup3_activation_dice_loss: 0.9739 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup3_activation_precision: 0.0229 - val_unet3plus_output_sup3_activation_recall: 0.0069 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0098 - val_unet3plus_output_sup4_activation_recall: 0.0021 - val_unet3plus_output_final_activation_dice_loss: 0.3003 - val_unet3plus_output_final_activation_iou_coeff: 0.5554 - val_unet3plus_output_final_activation_precision: 0.7720 - val_unet3plus_output_final_activation_recall: 0.7012 - val_unet3plusconcate_output1_activation_dice_loss: 0.9831 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0085 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-13T23:53:28.294227Z",
          "iopub.execute_input": "2021-07-13T23:53:28.294548Z",
          "iopub.status.idle": "2021-07-14T03:13:01.945983Z",
          "shell.execute_reply.started": "2021-07-13T23:53:28.294511Z",
          "shell.execute_reply": "2021-07-14T03:13:01.945176Z"
        },
        "trusted": true,
        "id": "xMzWoRYREHSr",
        "outputId": "b9883a36-c0a8-4402-b2eb-7bb0abcf8281"
      },
      "source": [
        "    model.fit(train_dataset,\n",
        "        validation_data=valid_dataset,\n",
        "        epochs=30,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_steps,\n",
        "        callbacks=callbacks,\n",
        "        shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/30\n350/350 [==============================] - 400s 1s/step - loss: 9.4490 - unet3plus_output_sup0_activation_loss: 1.4577 - unet3plus_output_sup1_activation_loss: 1.4972 - unet3plus_output_sup2_activation_loss: 1.6248 - unet3plus_output_sup3_activation_loss: 1.6349 - unet3plus_output_sup4_activation_loss: 1.6401 - unet3plus_output_final_activation_loss: 0.3675 - unet3plusconcate_output1_activation_loss: 1.2269 - unet3plus_output_sup0_activation_dice_loss: 0.9571 - unet3plus_output_sup0_activation_iou_coeff: 0.0220 - unet3plus_output_sup0_activation_precision: 0.0312 - unet3plus_output_sup0_activation_recall: 0.4869 - unet3plus_output_sup1_activation_dice_loss: 0.9634 - unet3plus_output_sup1_activation_iou_coeff: 0.0187 - unet3plus_output_sup1_activation_precision: 0.0546 - unet3plus_output_sup1_activation_recall: 0.2679 - unet3plus_output_sup2_activation_dice_loss: 0.9738 - unet3plus_output_sup2_activation_iou_coeff: 0.0133 - unet3plus_output_sup2_activation_precision: 0.0249 - unet3plus_output_sup2_activation_recall: 0.0399 - unet3plus_output_sup3_activation_dice_loss: 0.9744 - unet3plus_output_sup3_activation_iou_coeff: 0.0130 - unet3plus_output_sup3_activation_precision: 0.0385 - unet3plus_output_sup3_activation_recall: 0.0097 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.0096 - unet3plus_output_sup4_activation_recall: 0.0016 - unet3plus_output_final_activation_dice_loss: 0.2105 - unet3plus_output_final_activation_iou_coeff: 0.6600 - unet3plus_output_final_activation_precision: 0.8190 - unet3plus_output_final_activation_recall: 0.7967 - unet3plusconcate_output1_activation_dice_loss: 0.9851 - unet3plusconcate_output1_activation_iou_coeff: 0.0075 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 9.4921 - val_unet3plus_output_sup0_activation_loss: 1.4049 - val_unet3plus_output_sup1_activation_loss: 1.4471 - val_unet3plus_output_sup2_activation_loss: 1.6096 - val_unet3plus_output_sup3_activation_loss: 1.6244 - val_unet3plus_output_sup4_activation_loss: 1.6317 - val_unet3plus_output_final_activation_loss: 0.5574 - val_unet3plusconcate_output1_activation_loss: 1.2171 - val_unet3plus_output_sup0_activation_dice_loss: 0.9571 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0220 - val_unet3plus_output_sup0_activation_precision: 0.0253 - val_unet3plus_output_sup0_activation_recall: 0.4792 - val_unet3plus_output_sup1_activation_dice_loss: 0.9638 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0184 - val_unet3plus_output_sup1_activation_precision: 0.0446 - val_unet3plus_output_sup1_activation_recall: 0.2166 - val_unet3plus_output_sup2_activation_dice_loss: 0.9731 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup2_activation_precision: 0.0290 - val_unet3plus_output_sup2_activation_recall: 0.0370 - val_unet3plus_output_sup3_activation_dice_loss: 0.9738 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0133 - val_unet3plus_output_sup3_activation_precision: 0.0366 - val_unet3plus_output_sup3_activation_recall: 0.0113 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0091 - val_unet3plus_output_sup4_activation_recall: 0.0019 - val_unet3plus_output_final_activation_dice_loss: 0.3716 - val_unet3plus_output_final_activation_iou_coeff: 0.4913 - val_unet3plus_output_final_activation_precision: 0.8410 - val_unet3plus_output_final_activation_recall: 0.5508 - val_unet3plusconcate_output1_activation_dice_loss: 0.9837 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0082 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 2/30\n350/350 [==============================] - 396s 1s/step - loss: 9.2557 - unet3plus_output_sup0_activation_loss: 1.4086 - unet3plus_output_sup1_activation_loss: 1.4657 - unet3plus_output_sup2_activation_loss: 1.6040 - unet3plus_output_sup3_activation_loss: 1.6179 - unet3plus_output_sup4_activation_loss: 1.6248 - unet3plus_output_final_activation_loss: 0.3413 - unet3plusconcate_output1_activation_loss: 1.1933 - unet3plus_output_sup0_activation_dice_loss: 0.9513 - unet3plus_output_sup0_activation_iou_coeff: 0.0250 - unet3plus_output_sup0_activation_precision: 0.0317 - unet3plus_output_sup0_activation_recall: 0.4943 - unet3plus_output_sup1_activation_dice_loss: 0.9611 - unet3plus_output_sup1_activation_iou_coeff: 0.0199 - unet3plus_output_sup1_activation_precision: 0.0683 - unet3plus_output_sup1_activation_recall: 0.2800 - unet3plus_output_sup2_activation_dice_loss: 0.9734 - unet3plus_output_sup2_activation_iou_coeff: 0.0135 - unet3plus_output_sup2_activation_precision: 0.0440 - unet3plus_output_sup2_activation_recall: 0.0440 - unet3plus_output_sup3_activation_dice_loss: 0.9743 - unet3plus_output_sup3_activation_iou_coeff: 0.0130 - unet3plus_output_sup3_activation_precision: 0.0460 - unet3plus_output_sup3_activation_recall: 0.0119 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.0080 - unet3plus_output_sup4_activation_recall: 0.0014 - unet3plus_output_final_activation_dice_loss: 0.1929 - unet3plus_output_final_activation_iou_coeff: 0.6840 - unet3plus_output_final_activation_precision: 0.8337 - unet3plus_output_final_activation_recall: 0.8128 - unet3plusconcate_output1_activation_dice_loss: 0.9877 - unet3plusconcate_output1_activation_iou_coeff: 0.0062 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 9.1879 - val_unet3plus_output_sup0_activation_loss: 1.3516 - val_unet3plus_output_sup1_activation_loss: 1.4093 - val_unet3plus_output_sup2_activation_loss: 1.5868 - val_unet3plus_output_sup3_activation_loss: 1.6074 - val_unet3plus_output_sup4_activation_loss: 1.6163 - val_unet3plus_output_final_activation_loss: 0.4265 - val_unet3plusconcate_output1_activation_loss: 1.1899 - val_unet3plus_output_sup0_activation_dice_loss: 0.9477 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0269 - val_unet3plus_output_sup0_activation_precision: 0.0299 - val_unet3plus_output_sup0_activation_recall: 0.5676 - val_unet3plus_output_sup1_activation_dice_loss: 0.9569 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0221 - val_unet3plus_output_sup1_activation_precision: 0.0655 - val_unet3plus_output_sup1_activation_recall: 0.3274 - val_unet3plus_output_sup2_activation_dice_loss: 0.9727 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0139 - val_unet3plus_output_sup2_activation_precision: 0.0395 - val_unet3plus_output_sup2_activation_recall: 0.0482 - val_unet3plus_output_sup3_activation_dice_loss: 0.9736 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0134 - val_unet3plus_output_sup3_activation_precision: 0.0523 - val_unet3plus_output_sup3_activation_recall: 0.0167 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0075 - val_unet3plus_output_sup4_activation_recall: 0.0016 - val_unet3plus_output_final_activation_dice_loss: 0.2576 - val_unet3plus_output_final_activation_iou_coeff: 0.6061 - val_unet3plus_output_final_activation_precision: 0.8304 - val_unet3plus_output_final_activation_recall: 0.7166 - val_unet3plusconcate_output1_activation_dice_loss: 0.9882 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0060 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 3/30\n350/350 [==============================] - 396s 1s/step - loss: 9.0795 - unet3plus_output_sup0_activation_loss: 1.3676 - unet3plus_output_sup1_activation_loss: 1.4361 - unet3plus_output_sup2_activation_loss: 1.5835 - unet3plus_output_sup3_activation_loss: 1.6017 - unet3plus_output_sup4_activation_loss: 1.6099 - unet3plus_output_final_activation_loss: 0.3086 - unet3plusconcate_output1_activation_loss: 1.1720 - unet3plus_output_sup0_activation_dice_loss: 0.9452 - unet3plus_output_sup0_activation_iou_coeff: 0.0282 - unet3plus_output_sup0_activation_precision: 0.0325 - unet3plus_output_sup0_activation_recall: 0.5061 - unet3plus_output_sup1_activation_dice_loss: 0.9585 - unet3plus_output_sup1_activation_iou_coeff: 0.0212 - unet3plus_output_sup1_activation_precision: 0.0737 - unet3plus_output_sup1_activation_recall: 0.3031 - unet3plus_output_sup2_activation_dice_loss: 0.9730 - unet3plus_output_sup2_activation_iou_coeff: 0.0137 - unet3plus_output_sup2_activation_precision: 0.0515 - unet3plus_output_sup2_activation_recall: 0.0520 - unet3plus_output_sup3_activation_dice_loss: 0.9742 - unet3plus_output_sup3_activation_iou_coeff: 0.0131 - unet3plus_output_sup3_activation_precision: 0.0529 - unet3plus_output_sup3_activation_recall: 0.0137 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.0100 - unet3plus_output_sup4_activation_recall: 9.9617e-04 - unet3plus_output_final_activation_dice_loss: 0.1707 - unet3plus_output_final_activation_iou_coeff: 0.7138 - unet3plus_output_final_activation_precision: 0.8514 - unet3plus_output_final_activation_recall: 0.8358 - unet3plusconcate_output1_activation_dice_loss: 0.9900 - unet3plusconcate_output1_activation_iou_coeff: 0.0050 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 9.1242 - val_unet3plus_output_sup0_activation_loss: 1.2986 - val_unet3plus_output_sup1_activation_loss: 1.3753 - val_unet3plus_output_sup2_activation_loss: 1.5646 - val_unet3plus_output_sup3_activation_loss: 1.5907 - val_unet3plus_output_sup4_activation_loss: 1.6012 - val_unet3plus_output_final_activation_loss: 0.5137 - val_unet3plusconcate_output1_activation_loss: 1.1799 - val_unet3plus_output_sup0_activation_dice_loss: 0.9435 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0291 - val_unet3plus_output_sup0_activation_precision: 0.0253 - val_unet3plus_output_sup0_activation_recall: 0.4799 - val_unet3plus_output_sup1_activation_dice_loss: 0.9571 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0220 - val_unet3plus_output_sup1_activation_precision: 0.0551 - val_unet3plus_output_sup1_activation_recall: 0.2734 - val_unet3plus_output_sup2_activation_dice_loss: 0.9723 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0140 - val_unet3plus_output_sup2_activation_precision: 0.0415 - val_unet3plus_output_sup2_activation_recall: 0.0510 - val_unet3plus_output_sup3_activation_dice_loss: 0.9736 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0134 - val_unet3plus_output_sup3_activation_precision: 0.0499 - val_unet3plus_output_sup3_activation_recall: 0.0158 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0180 - val_unet3plus_output_sup4_activation_recall: 0.0013 - val_unet3plus_output_final_activation_dice_loss: 0.3271 - val_unet3plus_output_final_activation_iou_coeff: 0.5362 - val_unet3plus_output_final_activation_precision: 0.7981 - val_unet3plus_output_final_activation_recall: 0.6377 - val_unet3plusconcate_output1_activation_dice_loss: 0.9877 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0062 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 4/30\n350/350 [==============================] - 396s 1s/step - loss: 8.9287 - unet3plus_output_sup0_activation_loss: 1.3327 - unet3plus_output_sup1_activation_loss: 1.4082 - unet3plus_output_sup2_activation_loss: 1.5637 - unet3plus_output_sup3_activation_loss: 1.5862 - unet3plus_output_sup4_activation_loss: 1.5954 - unet3plus_output_final_activation_loss: 0.2847 - unet3plusconcate_output1_activation_loss: 1.1578 - unet3plus_output_sup0_activation_dice_loss: 0.9395 - unet3plus_output_sup0_activation_iou_coeff: 0.0313 - unet3plus_output_sup0_activation_precision: 0.0330 - unet3plus_output_sup0_activation_recall: 0.5134 - unet3plus_output_sup1_activation_dice_loss: 0.9558 - unet3plus_output_sup1_activation_iou_coeff: 0.0226 - unet3plus_output_sup1_activation_precision: 0.0764 - unet3plus_output_sup1_activation_recall: 0.3145 - unet3plus_output_sup2_activation_dice_loss: 0.9726 - unet3plus_output_sup2_activation_iou_coeff: 0.0139 - unet3plus_output_sup2_activation_precision: 0.0572 - unet3plus_output_sup2_activation_recall: 0.0581 - unet3plus_output_sup3_activation_dice_loss: 0.9741 - unet3plus_output_sup3_activation_iou_coeff: 0.0131 - unet3plus_output_sup3_activation_precision: 0.0567 - unet3plus_output_sup3_activation_recall: 0.0147 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.0174 - unet3plus_output_sup4_activation_recall: 0.0010 - unet3plus_output_final_activation_dice_loss: 0.1555 - unet3plus_output_final_activation_iou_coeff: 0.7360 - unet3plus_output_final_activation_precision: 0.8674 - unet3plus_output_final_activation_recall: 0.8470 - unet3plusconcate_output1_activation_dice_loss: 0.9917 - unet3plusconcate_output1_activation_iou_coeff: 0.0042 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 9.0513 - val_unet3plus_output_sup0_activation_loss: 1.2600 - val_unet3plus_output_sup1_activation_loss: 1.3454 - val_unet3plus_output_sup2_activation_loss: 1.5434 - val_unet3plus_output_sup3_activation_loss: 1.5747 - val_unet3plus_output_sup4_activation_loss: 1.5864 - val_unet3plus_output_final_activation_loss: 0.5773 - val_unet3plusconcate_output1_activation_loss: 1.1641 - val_unet3plus_output_sup0_activation_dice_loss: 0.9413 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0303 - val_unet3plus_output_sup0_activation_precision: 0.0238 - val_unet3plus_output_sup0_activation_recall: 0.4510 - val_unet3plus_output_sup1_activation_dice_loss: 0.9571 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0220 - val_unet3plus_output_sup1_activation_precision: 0.0477 - val_unet3plus_output_sup1_activation_recall: 0.2368 - val_unet3plus_output_sup2_activation_dice_loss: 0.9721 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0142 - val_unet3plus_output_sup2_activation_precision: 0.0406 - val_unet3plus_output_sup2_activation_recall: 0.0500 - val_unet3plus_output_sup3_activation_dice_loss: 0.9735 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0134 - val_unet3plus_output_sup3_activation_precision: 0.0505 - val_unet3plus_output_sup3_activation_recall: 0.0160 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0171 - val_unet3plus_output_sup4_activation_recall: 0.0013 - val_unet3plus_output_final_activation_dice_loss: 0.3881 - val_unet3plus_output_final_activation_iou_coeff: 0.4820 - val_unet3plus_output_final_activation_precision: 0.8254 - val_unet3plus_output_final_activation_recall: 0.5433 - val_unet3plusconcate_output1_activation_dice_loss: 0.9879 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0061 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 5/30\n350/350 [==============================] - 396s 1s/step - loss: 8.7938 - unet3plus_output_sup0_activation_loss: 1.3017 - unet3plus_output_sup1_activation_loss: 1.3815 - unet3plus_output_sup2_activation_loss: 1.5449 - unet3plus_output_sup3_activation_loss: 1.5713 - unet3plus_output_sup4_activation_loss: 1.5813 - unet3plus_output_final_activation_loss: 0.2651 - unet3plusconcate_output1_activation_loss: 1.1481 - unet3plus_output_sup0_activation_dice_loss: 0.9336 - unet3plus_output_sup0_activation_iou_coeff: 0.0344 - unet3plus_output_sup0_activation_precision: 0.0336 - unet3plus_output_sup0_activation_recall: 0.5205 - unet3plus_output_sup1_activation_dice_loss: 0.9529 - unet3plus_output_sup1_activation_iou_coeff: 0.0241 - unet3plus_output_sup1_activation_precision: 0.0782 - unet3plus_output_sup1_activation_recall: 0.3208 - unet3plus_output_sup2_activation_dice_loss: 0.9722 - unet3plus_output_sup2_activation_iou_coeff: 0.0141 - unet3plus_output_sup2_activation_precision: 0.0612 - unet3plus_output_sup2_activation_recall: 0.0624 - unet3plus_output_sup3_activation_dice_loss: 0.9741 - unet3plus_output_sup3_activation_iou_coeff: 0.0132 - unet3plus_output_sup3_activation_precision: 0.0582 - unet3plus_output_sup3_activation_recall: 0.0151 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.0188 - unet3plus_output_sup4_activation_recall: 0.0011 - unet3plus_output_final_activation_dice_loss: 0.1435 - unet3plus_output_final_activation_iou_coeff: 0.7540 - unet3plus_output_final_activation_precision: 0.8765 - unet3plus_output_final_activation_recall: 0.8607 - unet3plusconcate_output1_activation_dice_loss: 0.9932 - unet3plusconcate_output1_activation_iou_coeff: 0.0034 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 8.8478 - val_unet3plus_output_sup0_activation_loss: 1.2194 - val_unet3plus_output_sup1_activation_loss: 1.3121 - val_unet3plus_output_sup2_activation_loss: 1.5230 - val_unet3plus_output_sup3_activation_loss: 1.5594 - val_unet3plus_output_sup4_activation_loss: 1.5718 - val_unet3plus_output_final_activation_loss: 0.5030 - val_unet3plusconcate_output1_activation_loss: 1.1591 - val_unet3plus_output_sup0_activation_dice_loss: 0.9323 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0352 - val_unet3plus_output_sup0_activation_precision: 0.0238 - val_unet3plus_output_sup0_activation_recall: 0.4483 - val_unet3plus_output_sup1_activation_dice_loss: 0.9519 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0247 - val_unet3plus_output_sup1_activation_precision: 0.0529 - val_unet3plus_output_sup1_activation_recall: 0.2611 - val_unet3plus_output_sup2_activation_dice_loss: 0.9714 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0145 - val_unet3plus_output_sup2_activation_precision: 0.0498 - val_unet3plus_output_sup2_activation_recall: 0.0615 - val_unet3plus_output_sup3_activation_dice_loss: 0.9735 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0134 - val_unet3plus_output_sup3_activation_precision: 0.0480 - val_unet3plus_output_sup3_activation_recall: 0.0151 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0169 - val_unet3plus_output_sup4_activation_recall: 0.0013 - val_unet3plus_output_final_activation_dice_loss: 0.3137 - val_unet3plus_output_final_activation_iou_coeff: 0.5483 - val_unet3plus_output_final_activation_precision: 0.8493 - val_unet3plus_output_final_activation_recall: 0.6261 - val_unet3plusconcate_output1_activation_dice_loss: 0.9894 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0053 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 6/30\n350/350 [==============================] - 397s 1s/step - loss: 8.6672 - unet3plus_output_sup0_activation_loss: 1.2730 - unet3plus_output_sup1_activation_loss: 1.3554 - unet3plus_output_sup2_activation_loss: 1.5272 - unet3plus_output_sup3_activation_loss: 1.5569 - unet3plus_output_sup4_activation_loss: 1.5674 - unet3plus_output_final_activation_loss: 0.2457 - unet3plusconcate_output1_activation_loss: 1.1416 - unet3plus_output_sup0_activation_dice_loss: 0.9275 - unet3plus_output_sup0_activation_iou_coeff: 0.0377 - unet3plus_output_sup0_activation_precision: 0.0346 - unet3plus_output_sup0_activation_recall: 0.5262 - unet3plus_output_sup1_activation_dice_loss: 0.9497 - unet3plus_output_sup1_activation_iou_coeff: 0.0258 - unet3plus_output_sup1_activation_precision: 0.3847 - unet3plus_output_sup1_activation_recall: 0.3276 - unet3plus_output_sup2_activation_dice_loss: 0.9718 - unet3plus_output_sup2_activation_iou_coeff: 0.0143 - unet3plus_output_sup2_activation_precision: 0.0637 - unet3plus_output_sup2_activation_recall: 0.0649 - unet3plus_output_sup3_activation_dice_loss: 0.9740 - unet3plus_output_sup3_activation_iou_coeff: 0.0132 - unet3plus_output_sup3_activation_precision: 0.0613 - unet3plus_output_sup3_activation_recall: 0.0159 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.0196 - unet3plus_output_sup4_activation_recall: 0.0011 - unet3plus_output_final_activation_dice_loss: 0.1313 - unet3plus_output_final_activation_iou_coeff: 0.7721 - unet3plus_output_final_activation_precision: 0.8880 - unet3plus_output_final_activation_recall: 0.8718 - unet3plusconcate_output1_activation_dice_loss: 0.9943 - unet3plusconcate_output1_activation_iou_coeff: 0.0028 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 8.7076 - val_unet3plus_output_sup0_activation_loss: 1.1764 - val_unet3plus_output_sup1_activation_loss: 1.2827 - val_unet3plus_output_sup2_activation_loss: 1.5038 - val_unet3plus_output_sup3_activation_loss: 1.5446 - val_unet3plus_output_sup4_activation_loss: 1.5576 - val_unet3plus_output_final_activation_loss: 0.4879 - val_unet3plusconcate_output1_activation_loss: 1.1546 - val_unet3plus_output_sup0_activation_dice_loss: 0.9169 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0435 - val_unet3plus_output_sup0_activation_precision: 0.0261 - val_unet3plus_output_sup0_activation_recall: 0.4728 - val_unet3plus_output_sup1_activation_dice_loss: 0.9497 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0259 - val_unet3plus_output_sup1_activation_precision: 0.8607 - val_unet3plus_output_sup1_activation_recall: 0.2444 - val_unet3plus_output_sup2_activation_dice_loss: 0.9712 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0146 - val_unet3plus_output_sup2_activation_precision: 0.0460 - val_unet3plus_output_sup2_activation_recall: 0.0566 - val_unet3plus_output_sup3_activation_dice_loss: 0.9734 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0135 - val_unet3plus_output_sup3_activation_precision: 0.0523 - val_unet3plus_output_sup3_activation_recall: 0.0164 - val_unet3plus_output_sup4_activation_dice_loss: 0.9740 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.0160 - val_unet3plus_output_sup4_activation_recall: 0.0012 - val_unet3plus_output_final_activation_dice_loss: 0.2974 - val_unet3plus_output_final_activation_iou_coeff: 0.5619 - val_unet3plus_output_final_activation_precision: 0.8487 - val_unet3plus_output_final_activation_recall: 0.6498 - val_unet3plusconcate_output1_activation_dice_loss: 0.9906 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0047 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 7/30\n350/350 [==============================] - 396s 1s/step - loss: 8.4459 - unet3plus_output_sup0_activation_loss: 1.2134 - unet3plus_output_sup1_activation_loss: 1.3105 - unet3plus_output_sup2_activation_loss: 1.4946 - unet3plus_output_sup3_activation_loss: 1.5297 - unet3plus_output_sup4_activation_loss: 1.5410 - unet3plus_output_final_activation_loss: 0.2222 - unet3plusconcate_output1_activation_loss: 1.1345 - unet3plus_output_sup0_activation_dice_loss: 0.9125 - unet3plus_output_sup0_activation_iou_coeff: 0.0459 - unet3plus_output_sup0_activation_precision: 0.7842 - unet3plus_output_sup0_activation_recall: 0.5351 - unet3plus_output_sup1_activation_dice_loss: 0.9431 - unet3plus_output_sup1_activation_iou_coeff: 0.0294 - unet3plus_output_sup1_activation_precision: 0.8774 - unet3plus_output_sup1_activation_recall: 0.3652 - unet3plus_output_sup2_activation_dice_loss: 0.9710 - unet3plus_output_sup2_activation_iou_coeff: 0.0147 - unet3plus_output_sup2_activation_precision: 0.2598 - unet3plus_output_sup2_activation_recall: 0.0674 - unet3plus_output_sup3_activation_dice_loss: 0.9739 - unet3plus_output_sup3_activation_iou_coeff: 0.0133 - unet3plus_output_sup3_activation_precision: 0.0648 - unet3plus_output_sup3_activation_recall: 0.0164 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.2788 - unet3plus_output_sup4_activation_recall: 0.0011 - unet3plus_output_final_activation_dice_loss: 0.1176 - unet3plus_output_final_activation_iou_coeff: 0.7939 - unet3plus_output_final_activation_precision: 0.8999 - unet3plus_output_final_activation_recall: 0.8861 - unet3plusconcate_output1_activation_dice_loss: 0.9960 - unet3plusconcate_output1_activation_iou_coeff: 0.0020 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 8.6368 - val_unet3plus_output_sup0_activation_loss: 1.1202 - val_unet3plus_output_sup1_activation_loss: 1.2488 - val_unet3plus_output_sup2_activation_loss: 1.4692 - val_unet3plus_output_sup3_activation_loss: 1.5161 - val_unet3plus_output_sup4_activation_loss: 1.5303 - val_unet3plus_output_final_activation_loss: 0.6094 - val_unet3plusconcate_output1_activation_loss: 1.1427 - val_unet3plus_output_sup0_activation_dice_loss: 0.9115 - val_unet3plus_output_sup0_activation_iou_coeff: 0.0468 - val_unet3plus_output_sup0_activation_precision: 0.5628 - val_unet3plus_output_sup0_activation_recall: 0.2941 - val_unet3plus_output_sup1_activation_dice_loss: 0.9492 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0261 - val_unet3plus_output_sup1_activation_precision: 0.8643 - val_unet3plus_output_sup1_activation_recall: 0.2096 - val_unet3plus_output_sup2_activation_dice_loss: 0.9711 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0147 - val_unet3plus_output_sup2_activation_precision: 0.8684 - val_unet3plus_output_sup2_activation_recall: 0.0466 - val_unet3plus_output_sup3_activation_dice_loss: 0.9733 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup3_activation_precision: 0.0585 - val_unet3plus_output_sup3_activation_recall: 0.0170 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.2392 - val_unet3plus_output_sup4_activation_recall: 0.0011 - val_unet3plus_output_final_activation_dice_loss: 0.4101 - val_unet3plus_output_final_activation_iou_coeff: 0.4599 - val_unet3plus_output_final_activation_precision: 0.8476 - val_unet3plus_output_final_activation_recall: 0.5082 - val_unet3plusconcate_output1_activation_dice_loss: 0.9882 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0060 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 9/30\n350/350 [==============================] - 397s 1s/step - loss: 8.3158 - unet3plus_output_sup0_activation_loss: 1.1343 - unet3plus_output_sup1_activation_loss: 1.2992 - unet3plus_output_sup2_activation_loss: 1.4795 - unet3plus_output_sup3_activation_loss: 1.5167 - unet3plus_output_sup4_activation_loss: 1.5285 - unet3plus_output_final_activation_loss: 0.2219 - unet3plusconcate_output1_activation_loss: 1.1358 - unet3plus_output_sup0_activation_dice_loss: 0.8830 - unet3plus_output_sup0_activation_iou_coeff: 0.0624 - unet3plus_output_sup0_activation_precision: 0.7883 - unet3plus_output_sup0_activation_recall: 0.6092 - unet3plus_output_sup1_activation_dice_loss: 0.9411 - unet3plus_output_sup1_activation_iou_coeff: 0.0304 - unet3plus_output_sup1_activation_precision: 0.8759 - unet3plus_output_sup1_activation_recall: 0.3738 - unet3plus_output_sup2_activation_dice_loss: 0.9706 - unet3plus_output_sup2_activation_iou_coeff: 0.0149 - unet3plus_output_sup2_activation_precision: 0.8666 - unet3plus_output_sup2_activation_recall: 0.0682 - unet3plus_output_sup3_activation_dice_loss: 0.9738 - unet3plus_output_sup3_activation_iou_coeff: 0.0133 - unet3plus_output_sup3_activation_precision: 0.3308 - unet3plus_output_sup3_activation_recall: 0.0164 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.3461 - unet3plus_output_sup4_activation_recall: 0.0013 - unet3plus_output_final_activation_dice_loss: 0.1175 - unet3plus_output_final_activation_iou_coeff: 0.7942 - unet3plus_output_final_activation_precision: 0.9007 - unet3plus_output_final_activation_recall: 0.8863 - unet3plusconcate_output1_activation_dice_loss: 0.9966 - unet3plusconcate_output1_activation_iou_coeff: 0.0017 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 8.3556 - val_unet3plus_output_sup0_activation_loss: 0.9546 - val_unet3plus_output_sup1_activation_loss: 1.2323 - val_unet3plus_output_sup2_activation_loss: 1.4532 - val_unet3plus_output_sup3_activation_loss: 1.5024 - val_unet3plus_output_sup4_activation_loss: 1.5173 - val_unet3plus_output_final_activation_loss: 0.5444 - val_unet3plusconcate_output1_activation_loss: 1.1514 - val_unet3plus_output_sup0_activation_dice_loss: 0.7084 - val_unet3plus_output_sup0_activation_iou_coeff: 0.1783 - val_unet3plus_output_sup0_activation_precision: 0.5754 - val_unet3plus_output_sup0_activation_recall: 0.3945 - val_unet3plus_output_sup1_activation_dice_loss: 0.9423 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0298 - val_unet3plus_output_sup1_activation_precision: 0.8621 - val_unet3plus_output_sup1_activation_recall: 0.2643 - val_unet3plus_output_sup2_activation_dice_loss: 0.9708 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0148 - val_unet3plus_output_sup2_activation_precision: 0.8829 - val_unet3plus_output_sup2_activation_recall: 0.0474 - val_unet3plus_output_sup3_activation_dice_loss: 0.9732 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup3_activation_precision: 0.6427 - val_unet3plus_output_sup3_activation_recall: 0.0172 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.2743 - val_unet3plus_output_sup4_activation_recall: 0.0014 - val_unet3plus_output_final_activation_dice_loss: 0.3500 - val_unet3plus_output_final_activation_iou_coeff: 0.5176 - val_unet3plus_output_final_activation_precision: 0.8533 - val_unet3plus_output_final_activation_recall: 0.5850 - val_unet3plusconcate_output1_activation_dice_loss: 0.9902 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0049 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 10/30\n350/350 [==============================] - 396s 1s/step - loss: 8.1965 - unet3plus_output_sup0_activation_loss: 1.0779 - unet3plus_output_sup1_activation_loss: 1.2909 - unet3plus_output_sup2_activation_loss: 1.4659 - unet3plus_output_sup3_activation_loss: 1.5042 - unet3plus_output_sup4_activation_loss: 1.5163 - unet3plus_output_final_activation_loss: 0.2062 - unet3plusconcate_output1_activation_loss: 1.1352 - unet3plus_output_sup0_activation_dice_loss: 0.8495 - unet3plus_output_sup0_activation_iou_coeff: 0.0818 - unet3plus_output_sup0_activation_precision: 0.8084 - unet3plus_output_sup0_activation_recall: 0.7052 - unet3plus_output_sup1_activation_dice_loss: 0.9395 - unet3plus_output_sup1_activation_iou_coeff: 0.0313 - unet3plus_output_sup1_activation_precision: 0.8810 - unet3plus_output_sup1_activation_recall: 0.3824 - unet3plus_output_sup2_activation_dice_loss: 0.9702 - unet3plus_output_sup2_activation_iou_coeff: 0.0151 - unet3plus_output_sup2_activation_precision: 0.8697 - unet3plus_output_sup2_activation_recall: 0.0713 - unet3plus_output_sup3_activation_dice_loss: 0.9738 - unet3plus_output_sup3_activation_iou_coeff: 0.0133 - unet3plus_output_sup3_activation_precision: 0.6878 - unet3plus_output_sup3_activation_recall: 0.0166 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.3578 - unet3plus_output_sup4_activation_recall: 0.0014 - unet3plus_output_final_activation_dice_loss: 0.1083 - unet3plus_output_final_activation_iou_coeff: 0.8087 - unet3plus_output_final_activation_precision: 0.9096 - unet3plus_output_final_activation_recall: 0.8950 - unet3plusconcate_output1_activation_dice_loss: 0.9972 - unet3plusconcate_output1_activation_iou_coeff: 0.0014 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 8.0822 - val_unet3plus_output_sup0_activation_loss: 0.7731 - val_unet3plus_output_sup1_activation_loss: 1.2216 - val_unet3plus_output_sup2_activation_loss: 1.4390 - val_unet3plus_output_sup3_activation_loss: 1.4891 - val_unet3plus_output_sup4_activation_loss: 1.5046 - val_unet3plus_output_final_activation_loss: 0.5067 - val_unet3plusconcate_output1_activation_loss: 1.1481 - val_unet3plus_output_sup0_activation_dice_loss: 0.5484 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3050 - val_unet3plus_output_sup0_activation_precision: 0.6532 - val_unet3plus_output_sup0_activation_recall: 0.5414 - val_unet3plus_output_sup1_activation_dice_loss: 0.9388 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0316 - val_unet3plus_output_sup1_activation_precision: 0.8447 - val_unet3plus_output_sup1_activation_recall: 0.2879 - val_unet3plus_output_sup2_activation_dice_loss: 0.9698 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0153 - val_unet3plus_output_sup2_activation_precision: 0.8474 - val_unet3plus_output_sup2_activation_recall: 0.0624 - val_unet3plus_output_sup3_activation_dice_loss: 0.9732 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup3_activation_precision: 0.6393 - val_unet3plus_output_sup3_activation_recall: 0.0168 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup4_activation_precision: 0.2998 - val_unet3plus_output_sup4_activation_recall: 0.0014 - val_unet3plus_output_final_activation_dice_loss: 0.3171 - val_unet3plus_output_final_activation_iou_coeff: 0.5465 - val_unet3plus_output_final_activation_precision: 0.8121 - val_unet3plus_output_final_activation_recall: 0.6493 - val_unet3plusconcate_output1_activation_dice_loss: 0.9922 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0039 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 11/30\n350/350 [==============================] - 397s 1s/step - loss: 8.1265 - unet3plus_output_sup0_activation_loss: 1.0684 - unet3plus_output_sup1_activation_loss: 1.2839 - unet3plus_output_sup2_activation_loss: 1.4542 - unet3plus_output_sup3_activation_loss: 1.4921 - unet3plus_output_sup4_activation_loss: 1.5046 - unet3plus_output_final_activation_loss: 0.1928 - unet3plusconcate_output1_activation_loss: 1.1306 - unet3plus_output_sup0_activation_dice_loss: 0.8411 - unet3plus_output_sup0_activation_iou_coeff: 0.0867 - unet3plus_output_sup0_activation_precision: 0.8153 - unet3plus_output_sup0_activation_recall: 0.7214 - unet3plus_output_sup1_activation_dice_loss: 0.9382 - unet3plus_output_sup1_activation_iou_coeff: 0.0320 - unet3plus_output_sup1_activation_precision: 0.8876 - unet3plus_output_sup1_activation_recall: 0.3876 - unet3plus_output_sup2_activation_dice_loss: 0.9699 - unet3plus_output_sup2_activation_iou_coeff: 0.0153 - unet3plus_output_sup2_activation_precision: 0.8738 - unet3plus_output_sup2_activation_recall: 0.0749 - unet3plus_output_sup3_activation_dice_loss: 0.9737 - unet3plus_output_sup3_activation_iou_coeff: 0.0133 - unet3plus_output_sup3_activation_precision: 0.6942 - unet3plus_output_sup3_activation_recall: 0.0169 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.3850 - unet3plus_output_sup4_activation_recall: 0.0015 - unet3plus_output_final_activation_dice_loss: 0.1006 - unet3plus_output_final_activation_iou_coeff: 0.8209 - unet3plus_output_final_activation_precision: 0.9158 - unet3plus_output_final_activation_recall: 0.9034 - unet3plusconcate_output1_activation_dice_loss: 0.9975 - unet3plusconcate_output1_activation_iou_coeff: 0.0013 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.9332 - val_unet3plus_output_sup0_activation_loss: 0.7061 - val_unet3plus_output_sup1_activation_loss: 1.2163 - val_unet3plus_output_sup2_activation_loss: 1.4280 - val_unet3plus_output_sup3_activation_loss: 1.4764 - val_unet3plus_output_sup4_activation_loss: 1.4922 - val_unet3plus_output_final_activation_loss: 0.4733 - val_unet3plusconcate_output1_activation_loss: 1.1409 - val_unet3plus_output_sup0_activation_dice_loss: 0.4770 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3685 - val_unet3plus_output_sup0_activation_precision: 0.6950 - val_unet3plus_output_sup0_activation_recall: 0.5914 - val_unet3plus_output_sup1_activation_dice_loss: 0.9388 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0317 - val_unet3plus_output_sup1_activation_precision: 0.8654 - val_unet3plus_output_sup1_activation_recall: 0.2815 - val_unet3plus_output_sup2_activation_dice_loss: 0.9705 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0150 - val_unet3plus_output_sup2_activation_precision: 0.8857 - val_unet3plus_output_sup2_activation_recall: 0.0497 - val_unet3plus_output_sup3_activation_dice_loss: 0.9732 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup3_activation_precision: 0.7032 - val_unet3plus_output_sup3_activation_recall: 0.0151 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2975 - val_unet3plus_output_sup4_activation_recall: 0.0015 - val_unet3plus_output_final_activation_dice_loss: 0.2854 - val_unet3plus_output_final_activation_iou_coeff: 0.5762 - val_unet3plus_output_final_activation_precision: 0.8348 - val_unet3plus_output_final_activation_recall: 0.6738 - val_unet3plusconcate_output1_activation_dice_loss: 0.9923 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0039 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 12/30\n350/350 [==============================] - 396s 1s/step - loss: 8.0698 - unet3plus_output_sup0_activation_loss: 1.0644 - unet3plus_output_sup1_activation_loss: 1.2779 - unet3plus_output_sup2_activation_loss: 1.4435 - unet3plus_output_sup3_activation_loss: 1.4806 - unet3plus_output_sup4_activation_loss: 1.4931 - unet3plus_output_final_activation_loss: 0.1841 - unet3plusconcate_output1_activation_loss: 1.1261 - unet3plus_output_sup0_activation_dice_loss: 0.8374 - unet3plus_output_sup0_activation_iou_coeff: 0.0890 - unet3plus_output_sup0_activation_precision: 0.8183 - unet3plus_output_sup0_activation_recall: 0.7297 - unet3plus_output_sup1_activation_dice_loss: 0.9373 - unet3plus_output_sup1_activation_iou_coeff: 0.0325 - unet3plus_output_sup1_activation_precision: 0.8897 - unet3plus_output_sup1_activation_recall: 0.3903 - unet3plus_output_sup2_activation_dice_loss: 0.9697 - unet3plus_output_sup2_activation_iou_coeff: 0.0154 - unet3plus_output_sup2_activation_precision: 0.8768 - unet3plus_output_sup2_activation_recall: 0.0778 - unet3plus_output_sup3_activation_dice_loss: 0.9737 - unet3plus_output_sup3_activation_iou_coeff: 0.0134 - unet3plus_output_sup3_activation_precision: 0.6913 - unet3plus_output_sup3_activation_recall: 0.0177 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.3919 - unet3plus_output_sup4_activation_recall: 0.0015 - unet3plus_output_final_activation_dice_loss: 0.0958 - unet3plus_output_final_activation_iou_coeff: 0.8287 - unet3plus_output_final_activation_precision: 0.9188 - unet3plus_output_final_activation_recall: 0.9098 - unet3plusconcate_output1_activation_dice_loss: 0.9976 - unet3plusconcate_output1_activation_iou_coeff: 0.0012 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.8165 - val_unet3plus_output_sup0_activation_loss: 0.6492 - val_unet3plus_output_sup1_activation_loss: 1.2063 - val_unet3plus_output_sup2_activation_loss: 1.4167 - val_unet3plus_output_sup3_activation_loss: 1.4643 - val_unet3plus_output_sup4_activation_loss: 1.4801 - val_unet3plus_output_final_activation_loss: 0.4640 - val_unet3plusconcate_output1_activation_loss: 1.1360 - val_unet3plus_output_sup0_activation_dice_loss: 0.4287 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4157 - val_unet3plus_output_sup0_activation_precision: 0.7276 - val_unet3plus_output_sup0_activation_recall: 0.6145 - val_unet3plus_output_sup1_activation_dice_loss: 0.9348 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0338 - val_unet3plus_output_sup1_activation_precision: 0.8642 - val_unet3plus_output_sup1_activation_recall: 0.3055 - val_unet3plus_output_sup2_activation_dice_loss: 0.9701 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0152 - val_unet3plus_output_sup2_activation_precision: 0.8778 - val_unet3plus_output_sup2_activation_recall: 0.0550 - val_unet3plus_output_sup3_activation_dice_loss: 0.9731 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup3_activation_precision: 0.6893 - val_unet3plus_output_sup3_activation_recall: 0.0171 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2955 - val_unet3plus_output_sup4_activation_recall: 0.0015 - val_unet3plus_output_final_activation_dice_loss: 0.2789 - val_unet3plus_output_final_activation_iou_coeff: 0.5840 - val_unet3plus_output_final_activation_precision: 0.8598 - val_unet3plus_output_final_activation_recall: 0.6642 - val_unet3plusconcate_output1_activation_dice_loss: 0.9930 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0035 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 13/30\n350/350 [==============================] - 396s 1s/step - loss: 8.0181 - unet3plus_output_sup0_activation_loss: 1.0618 - unet3plus_output_sup1_activation_loss: 1.2720 - unet3plus_output_sup2_activation_loss: 1.4335 - unet3plus_output_sup3_activation_loss: 1.4697 - unet3plus_output_sup4_activation_loss: 1.4820 - unet3plus_output_final_activation_loss: 0.1772 - unet3plusconcate_output1_activation_loss: 1.1220 - unet3plus_output_sup0_activation_dice_loss: 0.8350 - unet3plus_output_sup0_activation_iou_coeff: 0.0904 - unet3plus_output_sup0_activation_precision: 0.8208 - unet3plus_output_sup0_activation_recall: 0.7347 - unet3plus_output_sup1_activation_dice_loss: 0.9364 - unet3plus_output_sup1_activation_iou_coeff: 0.0329 - unet3plus_output_sup1_activation_precision: 0.8922 - unet3plus_output_sup1_activation_recall: 0.3921 - unet3plus_output_sup2_activation_dice_loss: 0.9695 - unet3plus_output_sup2_activation_iou_coeff: 0.0155 - unet3plus_output_sup2_activation_precision: 0.8796 - unet3plus_output_sup2_activation_recall: 0.0792 - unet3plus_output_sup3_activation_dice_loss: 0.9736 - unet3plus_output_sup3_activation_iou_coeff: 0.0134 - unet3plus_output_sup3_activation_precision: 0.6997 - unet3plus_output_sup3_activation_recall: 0.0186 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.3971 - unet3plus_output_sup4_activation_recall: 0.0016 - unet3plus_output_final_activation_dice_loss: 0.0919 - unet3plus_output_final_activation_iou_coeff: 0.8351 - unet3plus_output_final_activation_precision: 0.9225 - unet3plus_output_final_activation_recall: 0.9138 - unet3plusconcate_output1_activation_dice_loss: 0.9976 - unet3plusconcate_output1_activation_iou_coeff: 0.0012 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.8705 - val_unet3plus_output_sup0_activation_loss: 0.6945 - val_unet3plus_output_sup1_activation_loss: 1.2027 - val_unet3plus_output_sup2_activation_loss: 1.4066 - val_unet3plus_output_sup3_activation_loss: 1.4527 - val_unet3plus_output_sup4_activation_loss: 1.4682 - val_unet3plus_output_final_activation_loss: 0.5190 - val_unet3plusconcate_output1_activation_loss: 1.1269 - val_unet3plus_output_sup0_activation_dice_loss: 0.4570 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3939 - val_unet3plus_output_sup0_activation_precision: 0.7147 - val_unet3plus_output_sup0_activation_recall: 0.5404 - val_unet3plus_output_sup1_activation_dice_loss: 0.9358 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0333 - val_unet3plus_output_sup1_activation_precision: 0.8825 - val_unet3plus_output_sup1_activation_recall: 0.2926 - val_unet3plus_output_sup2_activation_dice_loss: 0.9704 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0150 - val_unet3plus_output_sup2_activation_precision: 0.8727 - val_unet3plus_output_sup2_activation_recall: 0.0496 - val_unet3plus_output_sup3_activation_dice_loss: 0.9732 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup3_activation_precision: 0.7273 - val_unet3plus_output_sup3_activation_recall: 0.0150 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2991 - val_unet3plus_output_sup4_activation_recall: 0.0015 - val_unet3plus_output_final_activation_dice_loss: 0.3215 - val_unet3plus_output_final_activation_iou_coeff: 0.5412 - val_unet3plus_output_final_activation_precision: 0.8850 - val_unet3plus_output_final_activation_recall: 0.5954 - val_unet3plusconcate_output1_activation_dice_loss: 0.9919 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0041 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 14/30\n350/350 [==============================] - 396s 1s/step - loss: 7.9730 - unet3plus_output_sup0_activation_loss: 1.0600 - unet3plus_output_sup1_activation_loss: 1.2664 - unet3plus_output_sup2_activation_loss: 1.4238 - unet3plus_output_sup3_activation_loss: 1.4591 - unet3plus_output_sup4_activation_loss: 1.4711 - unet3plus_output_final_activation_loss: 0.1745 - unet3plusconcate_output1_activation_loss: 1.1182 - unet3plus_output_sup0_activation_dice_loss: 0.8333 - unet3plus_output_sup0_activation_iou_coeff: 0.0914 - unet3plus_output_sup0_activation_precision: 0.8214 - unet3plus_output_sup0_activation_recall: 0.7387 - unet3plus_output_sup1_activation_dice_loss: 0.9355 - unet3plus_output_sup1_activation_iou_coeff: 0.0334 - unet3plus_output_sup1_activation_precision: 0.8942 - unet3plus_output_sup1_activation_recall: 0.3946 - unet3plus_output_sup2_activation_dice_loss: 0.9694 - unet3plus_output_sup2_activation_iou_coeff: 0.0156 - unet3plus_output_sup2_activation_precision: 0.8845 - unet3plus_output_sup2_activation_recall: 0.0802 - unet3plus_output_sup3_activation_dice_loss: 0.9736 - unet3plus_output_sup3_activation_iou_coeff: 0.0134 - unet3plus_output_sup3_activation_precision: 0.6974 - unet3plus_output_sup3_activation_recall: 0.0190 - unet3plus_output_sup4_activation_dice_loss: 0.9746 - unet3plus_output_sup4_activation_iou_coeff: 0.0129 - unet3plus_output_sup4_activation_precision: 0.3983 - unet3plus_output_sup4_activation_recall: 0.0016 - unet3plus_output_final_activation_dice_loss: 0.0904 - unet3plus_output_final_activation_iou_coeff: 0.8375 - unet3plus_output_final_activation_precision: 0.9225 - unet3plus_output_final_activation_recall: 0.9175 - unet3plusconcate_output1_activation_dice_loss: 0.9975 - unet3plusconcate_output1_activation_iou_coeff: 0.0012 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.7077 - val_unet3plus_output_sup0_activation_loss: 0.6454 - val_unet3plus_output_sup1_activation_loss: 1.1922 - val_unet3plus_output_sup2_activation_loss: 1.3949 - val_unet3plus_output_sup3_activation_loss: 1.4412 - val_unet3plus_output_sup4_activation_loss: 1.4566 - val_unet3plus_output_final_activation_loss: 0.4523 - val_unet3plusconcate_output1_activation_loss: 1.1250 - val_unet3plus_output_sup0_activation_dice_loss: 0.4185 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4298 - val_unet3plus_output_sup0_activation_precision: 0.7164 - val_unet3plus_output_sup0_activation_recall: 0.6026 - val_unet3plus_output_sup1_activation_dice_loss: 0.9301 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0363 - val_unet3plus_output_sup1_activation_precision: 0.8525 - val_unet3plus_output_sup1_activation_recall: 0.3334 - val_unet3plus_output_sup2_activation_dice_loss: 0.9692 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0156 - val_unet3plus_output_sup2_activation_precision: 0.8551 - val_unet3plus_output_sup2_activation_recall: 0.0668 - val_unet3plus_output_sup3_activation_dice_loss: 0.9730 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0137 - val_unet3plus_output_sup3_activation_precision: 0.6654 - val_unet3plus_output_sup3_activation_recall: 0.0189 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2973 - val_unet3plus_output_sup4_activation_recall: 0.0014 - val_unet3plus_output_final_activation_dice_loss: 0.2662 - val_unet3plus_output_final_activation_iou_coeff: 0.5992 - val_unet3plus_output_final_activation_precision: 0.8385 - val_unet3plus_output_final_activation_recall: 0.6966 - val_unet3plusconcate_output1_activation_dice_loss: 0.9932 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0034 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 15/30\n350/350 [==============================] - 396s 1s/step - loss: 7.9237 - unet3plus_output_sup0_activation_loss: 1.0582 - unet3plus_output_sup1_activation_loss: 1.2609 - unet3plus_output_sup2_activation_loss: 1.4144 - unet3plus_output_sup3_activation_loss: 1.4488 - unet3plus_output_sup4_activation_loss: 1.4606 - unet3plus_output_final_activation_loss: 0.1665 - unet3plusconcate_output1_activation_loss: 1.1144 - unet3plus_output_sup0_activation_dice_loss: 0.8318 - unet3plus_output_sup0_activation_iou_coeff: 0.0924 - unet3plus_output_sup0_activation_precision: 0.8248 - unet3plus_output_sup0_activation_recall: 0.7434 - unet3plus_output_sup1_activation_dice_loss: 0.9348 - unet3plus_output_sup1_activation_iou_coeff: 0.0338 - unet3plus_output_sup1_activation_precision: 0.8973 - unet3plus_output_sup1_activation_recall: 0.3958 - unet3plus_output_sup2_activation_dice_loss: 0.9693 - unet3plus_output_sup2_activation_iou_coeff: 0.0156 - unet3plus_output_sup2_activation_precision: 0.8881 - unet3plus_output_sup2_activation_recall: 0.0797 - unet3plus_output_sup3_activation_dice_loss: 0.9736 - unet3plus_output_sup3_activation_iou_coeff: 0.0134 - unet3plus_output_sup3_activation_precision: 0.7033 - unet3plus_output_sup3_activation_recall: 0.0194 - unet3plus_output_sup4_activation_dice_loss: 0.9747 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.4036 - unet3plus_output_sup4_activation_recall: 0.0016 - unet3plus_output_final_activation_dice_loss: 0.0860 - unet3plus_output_final_activation_iou_coeff: 0.8447 - unet3plus_output_final_activation_precision: 0.9284 - unet3plus_output_final_activation_recall: 0.9202 - unet3plusconcate_output1_activation_dice_loss: 0.9974 - unet3plusconcate_output1_activation_iou_coeff: 0.0013 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.7314 - val_unet3plus_output_sup0_activation_loss: 0.6844 - val_unet3plus_output_sup1_activation_loss: 1.1863 - val_unet3plus_output_sup2_activation_loss: 1.3840 - val_unet3plus_output_sup3_activation_loss: 1.4303 - val_unet3plus_output_sup4_activation_loss: 1.4452 - val_unet3plus_output_final_activation_loss: 0.4808 - val_unet3plusconcate_output1_activation_loss: 1.1203 - val_unet3plus_output_sup0_activation_dice_loss: 0.4430 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4092 - val_unet3plus_output_sup0_activation_precision: 0.6476 - val_unet3plus_output_sup0_activation_recall: 0.6055 - val_unet3plus_output_sup1_activation_dice_loss: 0.9285 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0372 - val_unet3plus_output_sup1_activation_precision: 0.8310 - val_unet3plus_output_sup1_activation_recall: 0.3421 - val_unet3plus_output_sup2_activation_dice_loss: 0.9685 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0160 - val_unet3plus_output_sup2_activation_precision: 0.8421 - val_unet3plus_output_sup2_activation_recall: 0.0761 - val_unet3plus_output_sup3_activation_dice_loss: 0.9726 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0139 - val_unet3plus_output_sup3_activation_precision: 0.5303 - val_unet3plus_output_sup3_activation_recall: 0.0261 - val_unet3plus_output_sup4_activation_dice_loss: 0.9741 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2848 - val_unet3plus_output_sup4_activation_recall: 0.0013 - val_unet3plus_output_final_activation_dice_loss: 0.2852 - val_unet3plus_output_final_activation_iou_coeff: 0.5753 - val_unet3plus_output_final_activation_precision: 0.7812 - val_unet3plus_output_final_activation_recall: 0.7172 - val_unet3plusconcate_output1_activation_dice_loss: 0.9932 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0034 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 16/30\n350/350 [==============================] - 396s 1s/step - loss: 7.8768 - unet3plus_output_sup0_activation_loss: 1.0566 - unet3plus_output_sup1_activation_loss: 1.2554 - unet3plus_output_sup2_activation_loss: 1.4054 - unet3plus_output_sup3_activation_loss: 1.4389 - unet3plus_output_sup4_activation_loss: 1.4503 - unet3plus_output_final_activation_loss: 0.1595 - unet3plusconcate_output1_activation_loss: 1.1107 - unet3plus_output_sup0_activation_dice_loss: 0.8302 - unet3plus_output_sup0_activation_iou_coeff: 0.0933 - unet3plus_output_sup0_activation_precision: 0.8266 - unet3plus_output_sup0_activation_recall: 0.7475 - unet3plus_output_sup1_activation_dice_loss: 0.9338 - unet3plus_output_sup1_activation_iou_coeff: 0.0343 - unet3plus_output_sup1_activation_precision: 0.8982 - unet3plus_output_sup1_activation_recall: 0.3990 - unet3plus_output_sup2_activation_dice_loss: 0.9692 - unet3plus_output_sup2_activation_iou_coeff: 0.0157 - unet3plus_output_sup2_activation_precision: 0.8959 - unet3plus_output_sup2_activation_recall: 0.0793 - unet3plus_output_sup3_activation_dice_loss: 0.9735 - unet3plus_output_sup3_activation_iou_coeff: 0.0134 - unet3plus_output_sup3_activation_precision: 0.7065 - unet3plus_output_sup3_activation_recall: 0.0195 - unet3plus_output_sup4_activation_dice_loss: 0.9747 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.4186 - unet3plus_output_sup4_activation_recall: 0.0016 - unet3plus_output_final_activation_dice_loss: 0.0822 - unet3plus_output_final_activation_iou_coeff: 0.8513 - unet3plus_output_final_activation_precision: 0.9312 - unet3plus_output_final_activation_recall: 0.9243 - unet3plusconcate_output1_activation_dice_loss: 0.9971 - unet3plusconcate_output1_activation_iou_coeff: 0.0014 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 8.0505 - val_unet3plus_output_sup0_activation_loss: 0.8797 - val_unet3plus_output_sup1_activation_loss: 1.1953 - val_unet3plus_output_sup2_activation_loss: 1.3756 - val_unet3plus_output_sup3_activation_loss: 1.4193 - val_unet3plus_output_sup4_activation_loss: 1.4341 - val_unet3plus_output_final_activation_loss: 0.6389 - val_unet3plusconcate_output1_activation_loss: 1.1075 - val_unet3plus_output_sup0_activation_dice_loss: 0.5933 - val_unet3plus_output_sup0_activation_iou_coeff: 0.2834 - val_unet3plus_output_sup0_activation_precision: 0.5282 - val_unet3plus_output_sup0_activation_recall: 0.3958 - val_unet3plus_output_sup1_activation_dice_loss: 0.9394 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0313 - val_unet3plus_output_sup1_activation_precision: 0.8186 - val_unet3plus_output_sup1_activation_recall: 0.2543 - val_unet3plus_output_sup2_activation_dice_loss: 0.9698 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0154 - val_unet3plus_output_sup2_activation_precision: 0.8744 - val_unet3plus_output_sup2_activation_recall: 0.0560 - val_unet3plus_output_sup3_activation_dice_loss: 0.9729 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0138 - val_unet3plus_output_sup3_activation_precision: 0.6307 - val_unet3plus_output_sup3_activation_recall: 0.0212 - val_unet3plus_output_sup4_activation_dice_loss: 0.9742 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2797 - val_unet3plus_output_sup4_activation_recall: 0.0013 - val_unet3plus_output_final_activation_dice_loss: 0.4223 - val_unet3plus_output_final_activation_iou_coeff: 0.4468 - val_unet3plus_output_final_activation_precision: 0.7629 - val_unet3plus_output_final_activation_recall: 0.5383 - val_unet3plusconcate_output1_activation_dice_loss: 0.9903 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0049 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 17/30\n350/350 [==============================] - 397s 1s/step - loss: 7.8300 - unet3plus_output_sup0_activation_loss: 1.0552 - unet3plus_output_sup1_activation_loss: 1.2502 - unet3plus_output_sup2_activation_loss: 1.3966 - unet3plus_output_sup3_activation_loss: 1.4292 - unet3plus_output_sup4_activation_loss: 1.4404 - unet3plus_output_final_activation_loss: 0.1516 - unet3plusconcate_output1_activation_loss: 1.1069 - unet3plus_output_sup0_activation_dice_loss: 0.8289 - unet3plus_output_sup0_activation_iou_coeff: 0.0941 - unet3plus_output_sup0_activation_precision: 0.8287 - unet3plus_output_sup0_activation_recall: 0.7513 - unet3plus_output_sup1_activation_dice_loss: 0.9330 - unet3plus_output_sup1_activation_iou_coeff: 0.0347 - unet3plus_output_sup1_activation_precision: 0.9008 - unet3plus_output_sup1_activation_recall: 0.4003 - unet3plus_output_sup2_activation_dice_loss: 0.9692 - unet3plus_output_sup2_activation_iou_coeff: 0.0157 - unet3plus_output_sup2_activation_precision: 0.9021 - unet3plus_output_sup2_activation_recall: 0.0790 - unet3plus_output_sup3_activation_dice_loss: 0.9735 - unet3plus_output_sup3_activation_iou_coeff: 0.0134 - unet3plus_output_sup3_activation_precision: 0.7085 - unet3plus_output_sup3_activation_recall: 0.0200 - unet3plus_output_sup4_activation_dice_loss: 0.9747 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.4145 - unet3plus_output_sup4_activation_recall: 0.0015 - unet3plus_output_final_activation_dice_loss: 0.0778 - unet3plus_output_final_activation_iou_coeff: 0.8585 - unet3plus_output_final_activation_precision: 0.9356 - unet3plus_output_final_activation_recall: 0.9278 - unet3plusconcate_output1_activation_dice_loss: 0.9967 - unet3plusconcate_output1_activation_iou_coeff: 0.0016 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 8.0073 - val_unet3plus_output_sup0_activation_loss: 0.8554 - val_unet3plus_output_sup1_activation_loss: 1.1939 - val_unet3plus_output_sup2_activation_loss: 1.3658 - val_unet3plus_output_sup3_activation_loss: 1.4088 - val_unet3plus_output_sup4_activation_loss: 1.4232 - val_unet3plus_output_final_activation_loss: 0.6573 - val_unet3plusconcate_output1_activation_loss: 1.1029 - val_unet3plus_output_sup0_activation_dice_loss: 0.5742 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3038 - val_unet3plus_output_sup0_activation_precision: 0.5405 - val_unet3plus_output_sup0_activation_recall: 0.4173 - val_unet3plus_output_sup1_activation_dice_loss: 0.9415 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0302 - val_unet3plus_output_sup1_activation_precision: 0.8552 - val_unet3plus_output_sup1_activation_recall: 0.2313 - val_unet3plus_output_sup2_activation_dice_loss: 0.9695 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0155 - val_unet3plus_output_sup2_activation_precision: 0.8784 - val_unet3plus_output_sup2_activation_recall: 0.0586 - val_unet3plus_output_sup3_activation_dice_loss: 0.9728 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0138 - val_unet3plus_output_sup3_activation_precision: 0.6125 - val_unet3plus_output_sup3_activation_recall: 0.0229 - val_unet3plus_output_sup4_activation_dice_loss: 0.9742 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2870 - val_unet3plus_output_sup4_activation_recall: 0.0013 - val_unet3plus_output_final_activation_dice_loss: 0.4424 - val_unet3plus_output_final_activation_iou_coeff: 0.4315 - val_unet3plus_output_final_activation_precision: 0.7831 - val_unet3plus_output_final_activation_recall: 0.5096 - val_unet3plusconcate_output1_activation_dice_loss: 0.9899 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0051 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 18/30\n350/350 [==============================] - 396s 1s/step - loss: 7.7847 - unet3plus_output_sup0_activation_loss: 1.0538 - unet3plus_output_sup1_activation_loss: 1.2451 - unet3plus_output_sup2_activation_loss: 1.3880 - unet3plus_output_sup3_activation_loss: 1.4198 - unet3plus_output_sup4_activation_loss: 1.4307 - unet3plus_output_final_activation_loss: 0.1443 - unet3plusconcate_output1_activation_loss: 1.1030 - unet3plus_output_sup0_activation_dice_loss: 0.8275 - unet3plus_output_sup0_activation_iou_coeff: 0.0949 - unet3plus_output_sup0_activation_precision: 0.8314 - unet3plus_output_sup0_activation_recall: 0.7561 - unet3plus_output_sup1_activation_dice_loss: 0.9320 - unet3plus_output_sup1_activation_iou_coeff: 0.0353 - unet3plus_output_sup1_activation_precision: 0.9036 - unet3plus_output_sup1_activation_recall: 0.4033 - unet3plus_output_sup2_activation_dice_loss: 0.9691 - unet3plus_output_sup2_activation_iou_coeff: 0.0157 - unet3plus_output_sup2_activation_precision: 0.9123 - unet3plus_output_sup2_activation_recall: 0.0788 - unet3plus_output_sup3_activation_dice_loss: 0.9735 - unet3plus_output_sup3_activation_iou_coeff: 0.0134 - unet3plus_output_sup3_activation_precision: 0.7186 - unet3plus_output_sup3_activation_recall: 0.0203 - unet3plus_output_sup4_activation_dice_loss: 0.9747 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.4212 - unet3plus_output_sup4_activation_recall: 0.0015 - unet3plus_output_final_activation_dice_loss: 0.0739 - unet3plus_output_final_activation_iou_coeff: 0.8653 - unet3plus_output_final_activation_precision: 0.9388 - unet3plus_output_final_activation_recall: 0.9327 - unet3plusconcate_output1_activation_dice_loss: 0.9962 - unet3plusconcate_output1_activation_iou_coeff: 0.0019 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.7082 - val_unet3plus_output_sup0_activation_loss: 0.7117 - val_unet3plus_output_sup1_activation_loss: 1.1778 - val_unet3plus_output_sup2_activation_loss: 1.3568 - val_unet3plus_output_sup3_activation_loss: 1.3984 - val_unet3plus_output_sup4_activation_loss: 1.4126 - val_unet3plus_output_final_activation_loss: 0.5487 - val_unet3plusconcate_output1_activation_loss: 1.1021 - val_unet3plus_output_sup0_activation_dice_loss: 0.4616 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3977 - val_unet3plus_output_sup0_activation_precision: 0.6446 - val_unet3plus_output_sup0_activation_recall: 0.5518 - val_unet3plus_output_sup1_activation_dice_loss: 0.9323 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0352 - val_unet3plus_output_sup1_activation_precision: 0.8548 - val_unet3plus_output_sup1_activation_recall: 0.2957 - val_unet3plus_output_sup2_activation_dice_loss: 0.9697 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0154 - val_unet3plus_output_sup2_activation_precision: 0.8925 - val_unet3plus_output_sup2_activation_recall: 0.0547 - val_unet3plus_output_sup3_activation_dice_loss: 0.9728 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0138 - val_unet3plus_output_sup3_activation_precision: 0.6373 - val_unet3plus_output_sup3_activation_recall: 0.0219 - val_unet3plus_output_sup4_activation_dice_loss: 0.9742 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2804 - val_unet3plus_output_sup4_activation_recall: 0.0012 - val_unet3plus_output_final_activation_dice_loss: 0.3456 - val_unet3plus_output_final_activation_iou_coeff: 0.5207 - val_unet3plus_output_final_activation_precision: 0.8145 - val_unet3plus_output_final_activation_recall: 0.6170 - val_unet3plusconcate_output1_activation_dice_loss: 0.9910 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0045 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 19/30\n350/350 [==============================] - 396s 1s/step - loss: 7.7436 - unet3plus_output_sup0_activation_loss: 1.0525 - unet3plus_output_sup1_activation_loss: 1.2401 - unet3plus_output_sup2_activation_loss: 1.3800 - unet3plus_output_sup3_activation_loss: 1.4107 - unet3plus_output_sup4_activation_loss: 1.4213 - unet3plus_output_final_activation_loss: 0.1403 - unet3plusconcate_output1_activation_loss: 1.0986 - unet3plus_output_sup0_activation_dice_loss: 0.8263 - unet3plus_output_sup0_activation_iou_coeff: 0.0957 - unet3plus_output_sup0_activation_precision: 0.8314 - unet3plus_output_sup0_activation_recall: 0.7594 - unet3plus_output_sup1_activation_dice_loss: 0.9311 - unet3plus_output_sup1_activation_iou_coeff: 0.0358 - unet3plus_output_sup1_activation_precision: 0.9053 - unet3plus_output_sup1_activation_recall: 0.4052 - unet3plus_output_sup2_activation_dice_loss: 0.9692 - unet3plus_output_sup2_activation_iou_coeff: 0.0157 - unet3plus_output_sup2_activation_precision: 0.9191 - unet3plus_output_sup2_activation_recall: 0.0761 - unet3plus_output_sup3_activation_dice_loss: 0.9735 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7179 - unet3plus_output_sup3_activation_recall: 0.0206 - unet3plus_output_sup4_activation_dice_loss: 0.9747 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.4170 - unet3plus_output_sup4_activation_recall: 0.0014 - unet3plus_output_final_activation_dice_loss: 0.0718 - unet3plus_output_final_activation_iou_coeff: 0.8690 - unet3plus_output_final_activation_precision: 0.9403 - unet3plus_output_final_activation_recall: 0.9354 - unet3plusconcate_output1_activation_dice_loss: 0.9953 - unet3plusconcate_output1_activation_iou_coeff: 0.0024 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.7234 - val_unet3plus_output_sup0_activation_loss: 0.7315 - val_unet3plus_output_sup1_activation_loss: 1.1799 - val_unet3plus_output_sup2_activation_loss: 1.3485 - val_unet3plus_output_sup3_activation_loss: 1.3884 - val_unet3plus_output_sup4_activation_loss: 1.4022 - val_unet3plus_output_final_activation_loss: 0.5783 - val_unet3plusconcate_output1_activation_loss: 1.0945 - val_unet3plus_output_sup0_activation_dice_loss: 0.4812 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3796 - val_unet3plus_output_sup0_activation_precision: 0.6799 - val_unet3plus_output_sup0_activation_recall: 0.4952 - val_unet3plus_output_sup1_activation_dice_loss: 0.9392 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0315 - val_unet3plus_output_sup1_activation_precision: 0.8976 - val_unet3plus_output_sup1_activation_recall: 0.2383 - val_unet3plus_output_sup2_activation_dice_loss: 0.9704 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0150 - val_unet3plus_output_sup2_activation_precision: 0.9139 - val_unet3plus_output_sup2_activation_recall: 0.0441 - val_unet3plus_output_sup3_activation_dice_loss: 0.9731 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup3_activation_precision: 0.7512 - val_unet3plus_output_sup3_activation_recall: 0.0167 - val_unet3plus_output_sup4_activation_dice_loss: 0.9742 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2819 - val_unet3plus_output_sup4_activation_recall: 0.0012 - val_unet3plus_output_final_activation_dice_loss: 0.3705 - val_unet3plus_output_final_activation_iou_coeff: 0.4951 - val_unet3plus_output_final_activation_precision: 0.8689 - val_unet3plus_output_final_activation_recall: 0.5502 - val_unet3plusconcate_output1_activation_dice_loss: 0.9890 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0055 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 20/30\n350/350 [==============================] - 396s 1s/step - loss: 7.7078 - unet3plus_output_sup0_activation_loss: 1.0518 - unet3plus_output_sup1_activation_loss: 1.2354 - unet3plus_output_sup2_activation_loss: 1.3721 - unet3plus_output_sup3_activation_loss: 1.4019 - unet3plus_output_sup4_activation_loss: 1.4123 - unet3plus_output_final_activation_loss: 0.1405 - unet3plusconcate_output1_activation_loss: 1.0939 - unet3plus_output_sup0_activation_dice_loss: 0.8256 - unet3plus_output_sup0_activation_iou_coeff: 0.0961 - unet3plus_output_sup0_activation_precision: 0.8307 - unet3plus_output_sup0_activation_recall: 0.7611 - unet3plus_output_sup1_activation_dice_loss: 0.9304 - unet3plus_output_sup1_activation_iou_coeff: 0.0362 - unet3plus_output_sup1_activation_precision: 0.9024 - unet3plus_output_sup1_activation_recall: 0.4061 - unet3plus_output_sup2_activation_dice_loss: 0.9693 - unet3plus_output_sup2_activation_iou_coeff: 0.0156 - unet3plus_output_sup2_activation_precision: 0.9281 - unet3plus_output_sup2_activation_recall: 0.0736 - unet3plus_output_sup3_activation_dice_loss: 0.9734 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7267 - unet3plus_output_sup3_activation_recall: 0.0208 - unet3plus_output_sup4_activation_dice_loss: 0.9748 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.4254 - unet3plus_output_sup4_activation_recall: 0.0013 - unet3plus_output_final_activation_dice_loss: 0.0719 - unet3plus_output_final_activation_iou_coeff: 0.8688 - unet3plus_output_final_activation_precision: 0.9397 - unet3plus_output_final_activation_recall: 0.9358 - unet3plusconcate_output1_activation_dice_loss: 0.9940 - unet3plusconcate_output1_activation_iou_coeff: 0.0030 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.8307 - val_unet3plus_output_sup0_activation_loss: 0.8058 - val_unet3plus_output_sup1_activation_loss: 1.1767 - val_unet3plus_output_sup2_activation_loss: 1.3405 - val_unet3plus_output_sup3_activation_loss: 1.3785 - val_unet3plus_output_sup4_activation_loss: 1.3921 - val_unet3plus_output_final_activation_loss: 0.6476 - val_unet3plusconcate_output1_activation_loss: 1.0894 - val_unet3plus_output_sup0_activation_dice_loss: 0.5501 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3255 - val_unet3plus_output_sup0_activation_precision: 0.6126 - val_unet3plus_output_sup0_activation_recall: 0.4094 - val_unet3plus_output_sup1_activation_dice_loss: 0.9399 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0311 - val_unet3plus_output_sup1_activation_precision: 0.8830 - val_unet3plus_output_sup1_activation_recall: 0.2286 - val_unet3plus_output_sup2_activation_dice_loss: 0.9712 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0146 - val_unet3plus_output_sup2_activation_precision: 0.9209 - val_unet3plus_output_sup2_activation_recall: 0.0326 - val_unet3plus_output_sup3_activation_dice_loss: 0.9729 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0137 - val_unet3plus_output_sup3_activation_precision: 0.7058 - val_unet3plus_output_sup3_activation_recall: 0.0195 - val_unet3plus_output_sup4_activation_dice_loss: 0.9742 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup4_activation_precision: 0.2324 - val_unet3plus_output_sup4_activation_recall: 0.0010 - val_unet3plus_output_final_activation_dice_loss: 0.4406 - val_unet3plus_output_final_activation_iou_coeff: 0.4367 - val_unet3plus_output_final_activation_precision: 0.8466 - val_unet3plus_output_final_activation_recall: 0.4785 - val_unet3plusconcate_output1_activation_dice_loss: 0.9877 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0062 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 21/30\n350/350 [==============================] - 396s 1s/step - loss: 7.6706 - unet3plus_output_sup0_activation_loss: 1.0509 - unet3plus_output_sup1_activation_loss: 1.2309 - unet3plus_output_sup2_activation_loss: 1.3647 - unet3plus_output_sup3_activation_loss: 1.3934 - unet3plus_output_sup4_activation_loss: 1.4035 - unet3plus_output_final_activation_loss: 0.1387 - unet3plusconcate_output1_activation_loss: 1.0885 - unet3plus_output_sup0_activation_dice_loss: 0.8248 - unet3plus_output_sup0_activation_iou_coeff: 0.0966 - unet3plus_output_sup0_activation_precision: 0.8310 - unet3plus_output_sup0_activation_recall: 0.7638 - unet3plus_output_sup1_activation_dice_loss: 0.9296 - unet3plus_output_sup1_activation_iou_coeff: 0.0366 - unet3plus_output_sup1_activation_precision: 0.9029 - unet3plus_output_sup1_activation_recall: 0.4069 - unet3plus_output_sup2_activation_dice_loss: 0.9696 - unet3plus_output_sup2_activation_iou_coeff: 0.0155 - unet3plus_output_sup2_activation_precision: 0.9398 - unet3plus_output_sup2_activation_recall: 0.0679 - unet3plus_output_sup3_activation_dice_loss: 0.9734 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7230 - unet3plus_output_sup3_activation_recall: 0.0211 - unet3plus_output_sup4_activation_dice_loss: 0.9748 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.3980 - unet3plus_output_sup4_activation_recall: 0.0011 - unet3plus_output_final_activation_dice_loss: 0.0709 - unet3plus_output_final_activation_iou_coeff: 0.8705 - unet3plus_output_final_activation_precision: 0.9410 - unet3plus_output_final_activation_recall: 0.9369 - unet3plusconcate_output1_activation_dice_loss: 0.9921 - unet3plusconcate_output1_activation_iou_coeff: 0.0040 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.5390 - val_unet3plus_output_sup0_activation_loss: 0.6919 - val_unet3plus_output_sup1_activation_loss: 1.1605 - val_unet3plus_output_sup2_activation_loss: 1.3319 - val_unet3plus_output_sup3_activation_loss: 1.3689 - val_unet3plus_output_sup4_activation_loss: 1.3822 - val_unet3plus_output_final_activation_loss: 0.5169 - val_unet3plusconcate_output1_activation_loss: 1.0866 - val_unet3plus_output_sup0_activation_dice_loss: 0.4421 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4178 - val_unet3plus_output_sup0_activation_precision: 0.6943 - val_unet3plus_output_sup0_activation_recall: 0.5308 - val_unet3plus_output_sup1_activation_dice_loss: 0.9288 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0371 - val_unet3plus_output_sup1_activation_precision: 0.8759 - val_unet3plus_output_sup1_activation_recall: 0.3000 - val_unet3plus_output_sup2_activation_dice_loss: 0.9713 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0146 - val_unet3plus_output_sup2_activation_precision: 0.9034 - val_unet3plus_output_sup2_activation_recall: 0.0290 - val_unet3plus_output_sup3_activation_dice_loss: 0.9730 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0137 - val_unet3plus_output_sup3_activation_precision: 0.7261 - val_unet3plus_output_sup3_activation_recall: 0.0192 - val_unet3plus_output_sup4_activation_dice_loss: 0.9743 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup4_activation_precision: 0.1945 - val_unet3plus_output_sup4_activation_recall: 7.6812e-04 - val_unet3plus_output_final_activation_dice_loss: 0.3205 - val_unet3plus_output_final_activation_iou_coeff: 0.5457 - val_unet3plus_output_final_activation_precision: 0.8690 - val_unet3plus_output_final_activation_recall: 0.6154 - val_unet3plusconcate_output1_activation_dice_loss: 0.9875 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0063 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 22/30\n350/350 [==============================] - 396s 1s/step - loss: 7.6343 - unet3plus_output_sup0_activation_loss: 1.0502 - unet3plus_output_sup1_activation_loss: 1.2264 - unet3plus_output_sup2_activation_loss: 1.3578 - unet3plus_output_sup3_activation_loss: 1.3851 - unet3plus_output_sup4_activation_loss: 1.3950 - unet3plus_output_final_activation_loss: 0.1376 - unet3plusconcate_output1_activation_loss: 1.0820 - unet3plus_output_sup0_activation_dice_loss: 0.8241 - unet3plus_output_sup0_activation_iou_coeff: 0.0971 - unet3plus_output_sup0_activation_precision: 0.8297 - unet3plus_output_sup0_activation_recall: 0.7650 - unet3plus_output_sup1_activation_dice_loss: 0.9288 - unet3plus_output_sup1_activation_iou_coeff: 0.0370 - unet3plus_output_sup1_activation_precision: 0.9019 - unet3plus_output_sup1_activation_recall: 0.4086 - unet3plus_output_sup2_activation_dice_loss: 0.9703 - unet3plus_output_sup2_activation_iou_coeff: 0.0151 - unet3plus_output_sup2_activation_precision: 0.9525 - unet3plus_output_sup2_activation_recall: 0.0560 - unet3plus_output_sup3_activation_dice_loss: 0.9734 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7241 - unet3plus_output_sup3_activation_recall: 0.0211 - unet3plus_output_sup4_activation_dice_loss: 0.9748 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.3658 - unet3plus_output_sup4_activation_recall: 8.9249e-04 - unet3plus_output_final_activation_dice_loss: 0.0704 - unet3plus_output_final_activation_iou_coeff: 0.8714 - unet3plus_output_final_activation_precision: 0.9413 - unet3plus_output_final_activation_recall: 0.9376 - unet3plusconcate_output1_activation_dice_loss: 0.9890 - unet3plusconcate_output1_activation_iou_coeff: 0.0055 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.5997 - val_unet3plus_output_sup0_activation_loss: 0.7243 - val_unet3plus_output_sup1_activation_loss: 1.1594 - val_unet3plus_output_sup2_activation_loss: 1.3250 - val_unet3plus_output_sup3_activation_loss: 1.3595 - val_unet3plus_output_sup4_activation_loss: 1.3726 - val_unet3plus_output_final_activation_loss: 0.5789 - val_unet3plusconcate_output1_activation_loss: 1.0799 - val_unet3plus_output_sup0_activation_dice_loss: 0.4762 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3850 - val_unet3plus_output_sup0_activation_precision: 0.7078 - val_unet3plus_output_sup0_activation_recall: 0.4621 - val_unet3plus_output_sup1_activation_dice_loss: 0.9323 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0351 - val_unet3plus_output_sup1_activation_precision: 0.9080 - val_unet3plus_output_sup1_activation_recall: 0.2686 - val_unet3plus_output_sup2_activation_dice_loss: 0.9726 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0139 - val_unet3plus_output_sup2_activation_precision: 0.7495 - val_unet3plus_output_sup2_activation_recall: 0.0094 - val_unet3plus_output_sup3_activation_dice_loss: 0.9729 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0137 - val_unet3plus_output_sup3_activation_precision: 0.7100 - val_unet3plus_output_sup3_activation_recall: 0.0199 - val_unet3plus_output_sup4_activation_dice_loss: 0.9743 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup4_activation_precision: 0.1989 - val_unet3plus_output_sup4_activation_recall: 6.5646e-04 - val_unet3plus_output_final_activation_dice_loss: 0.3684 - val_unet3plus_output_final_activation_iou_coeff: 0.4936 - val_unet3plus_output_final_activation_precision: 0.9019 - val_unet3plus_output_final_activation_recall: 0.5331 - val_unet3plusconcate_output1_activation_dice_loss: 0.9845 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0078 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 23/30\n350/350 [==============================] - 396s 1s/step - loss: 7.5946 - unet3plus_output_sup0_activation_loss: 1.0491 - unet3plus_output_sup1_activation_loss: 1.2220 - unet3plus_output_sup2_activation_loss: 1.3521 - unet3plus_output_sup3_activation_loss: 1.3771 - unet3plus_output_sup4_activation_loss: 1.3867 - unet3plus_output_final_activation_loss: 0.1341 - unet3plusconcate_output1_activation_loss: 1.0735 - unet3plus_output_sup0_activation_dice_loss: 0.8230 - unet3plus_output_sup0_activation_iou_coeff: 0.0977 - unet3plus_output_sup0_activation_precision: 0.8341 - unet3plus_output_sup0_activation_recall: 0.7686 - unet3plus_output_sup1_activation_dice_loss: 0.9279 - unet3plus_output_sup1_activation_iou_coeff: 0.0375 - unet3plus_output_sup1_activation_precision: 0.9020 - unet3plus_output_sup1_activation_recall: 0.4108 - unet3plus_output_sup2_activation_dice_loss: 0.9718 - unet3plus_output_sup2_activation_iou_coeff: 0.0143 - unet3plus_output_sup2_activation_precision: 0.9481 - unet3plus_output_sup2_activation_recall: 0.0292 - unet3plus_output_sup3_activation_dice_loss: 0.9734 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7336 - unet3plus_output_sup3_activation_recall: 0.0213 - unet3plus_output_sup4_activation_dice_loss: 0.9748 - unet3plus_output_sup4_activation_iou_coeff: 0.0128 - unet3plus_output_sup4_activation_precision: 0.3042 - unet3plus_output_sup4_activation_recall: 5.5527e-04 - unet3plus_output_final_activation_dice_loss: 0.0685 - unet3plus_output_final_activation_iou_coeff: 0.8747 - unet3plus_output_final_activation_precision: 0.9427 - unet3plus_output_final_activation_recall: 0.9399 - unet3plusconcate_output1_activation_dice_loss: 0.9834 - unet3plusconcate_output1_activation_iou_coeff: 0.0084 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.4247 - val_unet3plus_output_sup0_activation_loss: 0.6348 - val_unet3plus_output_sup1_activation_loss: 1.1532 - val_unet3plus_output_sup2_activation_loss: 1.3174 - val_unet3plus_output_sup3_activation_loss: 1.3504 - val_unet3plus_output_sup4_activation_loss: 1.3633 - val_unet3plus_output_final_activation_loss: 0.5349 - val_unet3plusconcate_output1_activation_loss: 1.0710 - val_unet3plus_output_sup0_activation_dice_loss: 0.4038 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4560 - val_unet3plus_output_sup0_activation_precision: 0.8126 - val_unet3plus_output_sup0_activation_recall: 0.5494 - val_unet3plus_output_sup1_activation_dice_loss: 0.9307 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0360 - val_unet3plus_output_sup1_activation_precision: 0.8848 - val_unet3plus_output_sup1_activation_recall: 0.2760 - val_unet3plus_output_sup2_activation_dice_loss: 0.9732 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0136 - val_unet3plus_output_sup2_activation_precision: 0.5246 - val_unet3plus_output_sup2_activation_recall: 0.0018 - val_unet3plus_output_sup3_activation_dice_loss: 0.9729 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0138 - val_unet3plus_output_sup3_activation_precision: 0.7257 - val_unet3plus_output_sup3_activation_recall: 0.0200 - val_unet3plus_output_sup4_activation_dice_loss: 0.9744 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup4_activation_precision: 0.0929 - val_unet3plus_output_sup4_activation_recall: 3.5864e-04 - val_unet3plus_output_final_activation_dice_loss: 0.3358 - val_unet3plus_output_final_activation_iou_coeff: 0.5324 - val_unet3plus_output_final_activation_precision: 0.8689 - val_unet3plus_output_final_activation_recall: 0.5837 - val_unet3plusconcate_output1_activation_dice_loss: 0.9784 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0109 - val_unet3plusconcate_output1_activation_precision: 0.0000e+00 - val_unet3plusconcate_output1_activation_recall: 0.0000e+00\nEpoch 24/30\n350/350 [==============================] - 396s 1s/step - loss: 7.5654 - unet3plus_output_sup0_activation_loss: 1.0485 - unet3plus_output_sup1_activation_loss: 1.2182 - unet3plus_output_sup2_activation_loss: 1.3466 - unet3plus_output_sup3_activation_loss: 1.3694 - unet3plus_output_sup4_activation_loss: 1.3788 - unet3plus_output_final_activation_loss: 0.1416 - unet3plusconcate_output1_activation_loss: 1.0623 - unet3plus_output_sup0_activation_dice_loss: 0.8223 - unet3plus_output_sup0_activation_iou_coeff: 0.0981 - unet3plus_output_sup0_activation_precision: 0.8836 - unet3plus_output_sup0_activation_recall: 0.7671 - unet3plus_output_sup1_activation_dice_loss: 0.9274 - unet3plus_output_sup1_activation_iou_coeff: 0.0378 - unet3plus_output_sup1_activation_precision: 0.8952 - unet3plus_output_sup1_activation_recall: 0.4087 - unet3plus_output_sup2_activation_dice_loss: 0.9732 - unet3plus_output_sup2_activation_iou_coeff: 0.0136 - unet3plus_output_sup2_activation_precision: 0.8082 - unet3plus_output_sup2_activation_recall: 0.0059 - unet3plus_output_sup3_activation_dice_loss: 0.9734 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7147 - unet3plus_output_sup3_activation_recall: 0.0214 - unet3plus_output_sup4_activation_dice_loss: 0.9749 - unet3plus_output_sup4_activation_iou_coeff: 0.0127 - unet3plus_output_sup4_activation_precision: 0.1083 - unet3plus_output_sup4_activation_recall: 1.4426e-04 - unet3plus_output_final_activation_dice_loss: 0.0727 - unet3plus_output_final_activation_iou_coeff: 0.8677 - unet3plus_output_final_activation_precision: 0.9387 - unet3plus_output_final_activation_recall: 0.9357 - unet3plusconcate_output1_activation_dice_loss: 0.9736 - unet3plusconcate_output1_activation_iou_coeff: 0.0134 - unet3plusconcate_output1_activation_precision: 0.0000e+00 - unet3plusconcate_output1_activation_recall: 0.0000e+00 - val_loss: 7.3409 - val_unet3plus_output_sup0_activation_loss: 0.5850 - val_unet3plus_output_sup1_activation_loss: 1.1407 - val_unet3plus_output_sup2_activation_loss: 1.3106 - val_unet3plus_output_sup3_activation_loss: 1.3414 - val_unet3plus_output_sup4_activation_loss: 1.3541 - val_unet3plus_output_final_activation_loss: 0.5486 - val_unet3plusconcate_output1_activation_loss: 1.0605 - val_unet3plus_output_sup0_activation_dice_loss: 0.3683 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4932 - val_unet3plus_output_sup0_activation_precision: 0.8348 - val_unet3plus_output_sup0_activation_recall: 0.5871 - val_unet3plus_output_sup1_activation_dice_loss: 0.9230 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0402 - val_unet3plus_output_sup1_activation_precision: 0.8748 - val_unet3plus_output_sup1_activation_recall: 0.3184 - val_unet3plus_output_sup2_activation_dice_loss: 0.9738 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0133 - val_unet3plus_output_sup2_activation_precision: 0.0233 - val_unet3plus_output_sup2_activation_recall: 1.9492e-05 - val_unet3plus_output_sup3_activation_dice_loss: 0.9729 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0138 - val_unet3plus_output_sup3_activation_precision: 0.7181 - val_unet3plus_output_sup3_activation_recall: 0.0200 - val_unet3plus_output_sup4_activation_dice_loss: 0.9744 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup4_activation_precision: 0.0297 - val_unet3plus_output_sup4_activation_recall: 7.5675e-05 - val_unet3plus_output_final_activation_dice_loss: 0.3422 - val_unet3plus_output_final_activation_iou_coeff: 0.5215 - val_unet3plus_output_final_activation_precision: 0.8852 - val_unet3plus_output_final_activation_recall: 0.5811 - val_unet3plusconcate_output1_activation_dice_loss: 0.9684 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0161 - val_unet3plusconcate_output1_activation_precision: 0.1067 - val_unet3plusconcate_output1_activation_recall: 1.6990e-05\nEpoch 25/30\n350/350 [==============================] - 396s 1s/step - loss: 7.5112 - unet3plus_output_sup0_activation_loss: 1.0468 - unet3plus_output_sup1_activation_loss: 1.2138 - unet3plus_output_sup2_activation_loss: 1.3411 - unet3plus_output_sup3_activation_loss: 1.3618 - unet3plus_output_sup4_activation_loss: 1.3711 - unet3plus_output_final_activation_loss: 0.1292 - unet3plusconcate_output1_activation_loss: 1.0475 - unet3plus_output_sup0_activation_dice_loss: 0.8206 - unet3plus_output_sup0_activation_iou_coeff: 0.0991 - unet3plus_output_sup0_activation_precision: 0.8873 - unet3plus_output_sup0_activation_recall: 0.7730 - unet3plus_output_sup1_activation_dice_loss: 0.9262 - unet3plus_output_sup1_activation_iou_coeff: 0.0384 - unet3plus_output_sup1_activation_precision: 0.8968 - unet3plus_output_sup1_activation_recall: 0.4136 - unet3plus_output_sup2_activation_dice_loss: 0.9741 - unet3plus_output_sup2_activation_iou_coeff: 0.0132 - unet3plus_output_sup2_activation_precision: 0.2360 - unet3plus_output_sup2_activation_recall: 3.6716e-04 - unet3plus_output_sup3_activation_dice_loss: 0.9733 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7279 - unet3plus_output_sup3_activation_recall: 0.0218 - unet3plus_output_sup4_activation_dice_loss: 0.9749 - unet3plus_output_sup4_activation_iou_coeff: 0.0127 - unet3plus_output_sup4_activation_precision: 0.0261 - unet3plus_output_sup4_activation_recall: 2.1495e-05 - unet3plus_output_final_activation_dice_loss: 0.0660 - unet3plus_output_final_activation_iou_coeff: 0.8791 - unet3plus_output_final_activation_precision: 0.9454 - unet3plus_output_final_activation_recall: 0.9424 - unet3plusconcate_output1_activation_dice_loss: 0.9566 - unet3plusconcate_output1_activation_iou_coeff: 0.0222 - unet3plusconcate_output1_activation_precision: 0.0171 - unet3plusconcate_output1_activation_recall: 2.1576e-06 - val_loss: 7.3313 - val_unet3plus_output_sup0_activation_loss: 0.6208 - val_unet3plus_output_sup1_activation_loss: 1.1407 - val_unet3plus_output_sup2_activation_loss: 1.3031 - val_unet3plus_output_sup3_activation_loss: 1.3327 - val_unet3plus_output_sup4_activation_loss: 1.3453 - val_unet3plus_output_final_activation_loss: 0.5420 - val_unet3plusconcate_output1_activation_loss: 1.0467 - val_unet3plus_output_sup0_activation_dice_loss: 0.4019 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4619 - val_unet3plus_output_sup0_activation_precision: 0.8149 - val_unet3plus_output_sup0_activation_recall: 0.5555 - val_unet3plus_output_sup1_activation_dice_loss: 0.9255 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0389 - val_unet3plus_output_sup1_activation_precision: 0.8787 - val_unet3plus_output_sup1_activation_recall: 0.2978 - val_unet3plus_output_sup2_activation_dice_loss: 0.9740 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0132 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9728 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0138 - val_unet3plus_output_sup3_activation_precision: 0.7214 - val_unet3plus_output_sup3_activation_recall: 0.0204 - val_unet3plus_output_sup4_activation_dice_loss: 0.9745 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0129 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.3387 - val_unet3plus_output_final_activation_iou_coeff: 0.5275 - val_unet3plus_output_final_activation_precision: 0.8473 - val_unet3plus_output_final_activation_recall: 0.5964 - val_unet3plusconcate_output1_activation_dice_loss: 0.9517 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0248 - val_unet3plusconcate_output1_activation_precision: 0.5724 - val_unet3plusconcate_output1_activation_recall: 2.5000e-04\nEpoch 26/30\n350/350 [==============================] - 396s 1s/step - loss: 7.4577 - unet3plus_output_sup0_activation_loss: 1.0458 - unet3plus_output_sup1_activation_loss: 1.2095 - unet3plus_output_sup2_activation_loss: 1.3354 - unet3plus_output_sup3_activation_loss: 1.3545 - unet3plus_output_sup4_activation_loss: 1.3637 - unet3plus_output_final_activation_loss: 0.1217 - unet3plusconcate_output1_activation_loss: 1.0271 - unet3plus_output_sup0_activation_dice_loss: 0.8195 - unet3plus_output_sup0_activation_iou_coeff: 0.0999 - unet3plus_output_sup0_activation_precision: 0.8859 - unet3plus_output_sup0_activation_recall: 0.7776 - unet3plus_output_sup1_activation_dice_loss: 0.9249 - unet3plus_output_sup1_activation_iou_coeff: 0.0391 - unet3plus_output_sup1_activation_precision: 0.8913 - unet3plus_output_sup1_activation_recall: 0.4188 - unet3plus_output_sup2_activation_dice_loss: 0.9745 - unet3plus_output_sup2_activation_iou_coeff: 0.0129 - unet3plus_output_sup2_activation_precision: 0.0057 - unet3plus_output_sup2_activation_recall: 1.8287e-06 - unet3plus_output_sup3_activation_dice_loss: 0.9733 - unet3plus_output_sup3_activation_iou_coeff: 0.0135 - unet3plus_output_sup3_activation_precision: 0.7331 - unet3plus_output_sup3_activation_recall: 0.0220 - unet3plus_output_sup4_activation_dice_loss: 0.9750 - unet3plus_output_sup4_activation_iou_coeff: 0.0127 - unet3plus_output_sup4_activation_precision: 0.0057 - unet3plus_output_sup4_activation_recall: 2.0920e-06 - unet3plus_output_final_activation_dice_loss: 0.0620 - unet3plus_output_final_activation_iou_coeff: 0.8861 - unet3plus_output_final_activation_precision: 0.9491 - unet3plus_output_final_activation_recall: 0.9463 - unet3plusconcate_output1_activation_dice_loss: 0.9290 - unet3plusconcate_output1_activation_iou_coeff: 0.0369 - unet3plusconcate_output1_activation_precision: 0.4175 - unet3plusconcate_output1_activation_recall: 2.5804e-04 - val_loss: 7.2608 - val_unet3plus_output_sup0_activation_loss: 0.6088 - val_unet3plus_output_sup1_activation_loss: 1.1349 - val_unet3plus_output_sup2_activation_loss: 1.2960 - val_unet3plus_output_sup3_activation_loss: 1.3242 - val_unet3plus_output_sup4_activation_loss: 1.3367 - val_unet3plus_output_final_activation_loss: 0.5340 - val_unet3plusconcate_output1_activation_loss: 1.0261 - val_unet3plus_output_sup0_activation_dice_loss: 0.3905 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4718 - val_unet3plus_output_sup0_activation_precision: 0.8266 - val_unet3plus_output_sup0_activation_recall: 0.5638 - val_unet3plus_output_sup1_activation_dice_loss: 0.9238 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0398 - val_unet3plus_output_sup1_activation_precision: 0.8780 - val_unet3plus_output_sup1_activation_recall: 0.3032 - val_unet3plus_output_sup2_activation_dice_loss: 0.9743 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0131 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9727 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0138 - val_unet3plus_output_sup3_activation_precision: 0.6955 - val_unet3plus_output_sup3_activation_recall: 0.0220 - val_unet3plus_output_sup4_activation_dice_loss: 0.9745 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0129 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.3297 - val_unet3plus_output_final_activation_iou_coeff: 0.5348 - val_unet3plus_output_final_activation_precision: 0.8695 - val_unet3plus_output_final_activation_recall: 0.6063 - val_unet3plusconcate_output1_activation_dice_loss: 0.9235 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0400 - val_unet3plusconcate_output1_activation_precision: 0.7469 - val_unet3plusconcate_output1_activation_recall: 0.0025\nEpoch 27/30\n350/350 [==============================] - 396s 1s/step - loss: 7.4026 - unet3plus_output_sup0_activation_loss: 1.0454 - unet3plus_output_sup1_activation_loss: 1.2055 - unet3plus_output_sup2_activation_loss: 1.3297 - unet3plus_output_sup3_activation_loss: 1.3475 - unet3plus_output_sup4_activation_loss: 1.3566 - unet3plus_output_final_activation_loss: 0.1179 - unet3plusconcate_output1_activation_loss: 1.0000 - unet3plus_output_sup0_activation_dice_loss: 0.8187 - unet3plus_output_sup0_activation_iou_coeff: 0.1003 - unet3plus_output_sup0_activation_precision: 0.8824 - unet3plus_output_sup0_activation_recall: 0.7801 - unet3plus_output_sup1_activation_dice_loss: 0.9236 - unet3plus_output_sup1_activation_iou_coeff: 0.0398 - unet3plus_output_sup1_activation_precision: 0.8827 - unet3plus_output_sup1_activation_recall: 0.4233 - unet3plus_output_sup2_activation_dice_loss: 0.9747 - unet3plus_output_sup2_activation_iou_coeff: 0.0128 - unet3plus_output_sup2_activation_precision: 0.0000e+00 - unet3plus_output_sup2_activation_recall: 0.0000e+00 - unet3plus_output_sup3_activation_dice_loss: 0.9733 - unet3plus_output_sup3_activation_iou_coeff: 0.0136 - unet3plus_output_sup3_activation_precision: 0.7368 - unet3plus_output_sup3_activation_recall: 0.0221 - unet3plus_output_sup4_activation_dice_loss: 0.9750 - unet3plus_output_sup4_activation_iou_coeff: 0.0127 - unet3plus_output_sup4_activation_precision: 0.0000e+00 - unet3plus_output_sup4_activation_recall: 0.0000e+00 - unet3plus_output_final_activation_dice_loss: 0.0600 - unet3plus_output_final_activation_iou_coeff: 0.8897 - unet3plus_output_final_activation_precision: 0.9508 - unet3plus_output_final_activation_recall: 0.9487 - unet3plusconcate_output1_activation_dice_loss: 0.8901 - unet3plusconcate_output1_activation_iou_coeff: 0.0584 - unet3plusconcate_output1_activation_precision: 0.8138 - unet3plusconcate_output1_activation_recall: 0.0071 - val_loss: 7.5287 - val_unet3plus_output_sup0_activation_loss: 0.7496 - val_unet3plus_output_sup1_activation_loss: 1.1478 - val_unet3plus_output_sup2_activation_loss: 1.2891 - val_unet3plus_output_sup3_activation_loss: 1.3160 - val_unet3plus_output_sup4_activation_loss: 1.3284 - val_unet3plus_output_final_activation_loss: 0.6762 - val_unet3plusconcate_output1_activation_loss: 1.0216 - val_unet3plus_output_sup0_activation_dice_loss: 0.5176 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3591 - val_unet3plus_output_sup0_activation_precision: 0.8419 - val_unet3plus_output_sup0_activation_recall: 0.3984 - val_unet3plus_output_sup1_activation_dice_loss: 0.9377 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0323 - val_unet3plus_output_sup1_activation_precision: 0.9082 - val_unet3plus_output_sup1_activation_recall: 0.2106 - val_unet3plus_output_sup2_activation_dice_loss: 0.9744 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9729 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0137 - val_unet3plus_output_sup3_activation_precision: 0.7442 - val_unet3plus_output_sup3_activation_recall: 0.0189 - val_unet3plus_output_sup4_activation_dice_loss: 0.9745 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0129 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.4488 - val_unet3plus_output_final_activation_iou_coeff: 0.4202 - val_unet3plus_output_final_activation_precision: 0.8953 - val_unet3plus_output_final_activation_recall: 0.4548 - val_unet3plusconcate_output1_activation_dice_loss: 0.9155 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0448 - val_unet3plusconcate_output1_activation_precision: 0.7813 - val_unet3plusconcate_output1_activation_recall: 0.0243\nEpoch 28/30\n350/350 [==============================] - 396s 1s/step - loss: 7.3442 - unet3plus_output_sup0_activation_loss: 1.0451 - unet3plus_output_sup1_activation_loss: 1.2017 - unet3plus_output_sup2_activation_loss: 1.3242 - unet3plus_output_sup3_activation_loss: 1.3407 - unet3plus_output_sup4_activation_loss: 1.3499 - unet3plus_output_final_activation_loss: 0.1149 - unet3plusconcate_output1_activation_loss: 0.9678 - unet3plus_output_sup0_activation_dice_loss: 0.8182 - unet3plus_output_sup0_activation_iou_coeff: 0.1007 - unet3plus_output_sup0_activation_precision: 0.8787 - unet3plus_output_sup0_activation_recall: 0.7829 - unet3plus_output_sup1_activation_dice_loss: 0.9223 - unet3plus_output_sup1_activation_iou_coeff: 0.0406 - unet3plus_output_sup1_activation_precision: 0.8720 - unet3plus_output_sup1_activation_recall: 0.4277 - unet3plus_output_sup2_activation_dice_loss: 0.9748 - unet3plus_output_sup2_activation_iou_coeff: 0.0128 - unet3plus_output_sup2_activation_precision: 0.0000e+00 - unet3plus_output_sup2_activation_recall: 0.0000e+00 - unet3plus_output_sup3_activation_dice_loss: 0.9732 - unet3plus_output_sup3_activation_iou_coeff: 0.0136 - unet3plus_output_sup3_activation_precision: 0.7368 - unet3plus_output_sup3_activation_recall: 0.0222 - unet3plus_output_sup4_activation_dice_loss: 0.9750 - unet3plus_output_sup4_activation_iou_coeff: 0.0126 - unet3plus_output_sup4_activation_precision: 0.0000e+00 - unet3plus_output_sup4_activation_recall: 0.0000e+00 - unet3plus_output_final_activation_dice_loss: 0.0584 - unet3plus_output_final_activation_iou_coeff: 0.8925 - unet3plus_output_final_activation_precision: 0.9521 - unet3plus_output_final_activation_recall: 0.9506 - unet3plusconcate_output1_activation_dice_loss: 0.8441 - unet3plusconcate_output1_activation_iou_coeff: 0.0849 - unet3plusconcate_output1_activation_precision: 0.8609 - unet3plusconcate_output1_activation_recall: 0.0662 - val_loss: 7.3958 - val_unet3plus_output_sup0_activation_loss: 0.7237 - val_unet3plus_output_sup1_activation_loss: 1.1301 - val_unet3plus_output_sup2_activation_loss: 1.2821 - val_unet3plus_output_sup3_activation_loss: 1.3078 - val_unet3plus_output_sup4_activation_loss: 1.3206 - val_unet3plus_output_final_activation_loss: 0.6379 - val_unet3plusconcate_output1_activation_loss: 0.9936 - val_unet3plus_output_sup0_activation_dice_loss: 0.4809 - val_unet3plus_output_sup0_activation_iou_coeff: 0.3914 - val_unet3plus_output_sup0_activation_precision: 0.8590 - val_unet3plus_output_sup0_activation_recall: 0.4340 - val_unet3plus_output_sup1_activation_dice_loss: 0.9246 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0394 - val_unet3plus_output_sup1_activation_precision: 0.8952 - val_unet3plus_output_sup1_activation_recall: 0.2850 - val_unet3plus_output_sup2_activation_dice_loss: 0.9744 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9727 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0139 - val_unet3plus_output_sup3_activation_precision: 0.7061 - val_unet3plus_output_sup3_activation_recall: 0.0218 - val_unet3plus_output_sup4_activation_dice_loss: 0.9746 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0129 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.4115 - val_unet3plus_output_final_activation_iou_coeff: 0.4542 - val_unet3plus_output_final_activation_precision: 0.9157 - val_unet3plus_output_final_activation_recall: 0.4871 - val_unet3plusconcate_output1_activation_dice_loss: 0.8767 - val_unet3plusconcate_output1_activation_iou_coeff: 0.0671 - val_unet3plusconcate_output1_activation_precision: 0.8206 - val_unet3plusconcate_output1_activation_recall: 0.1287\nEpoch 29/30\n350/350 [==============================] - 396s 1s/step - loss: 7.2859 - unet3plus_output_sup0_activation_loss: 1.0449 - unet3plus_output_sup1_activation_loss: 1.1981 - unet3plus_output_sup2_activation_loss: 1.3189 - unet3plus_output_sup3_activation_loss: 1.3341 - unet3plus_output_sup4_activation_loss: 1.3437 - unet3plus_output_final_activation_loss: 0.1138 - unet3plusconcate_output1_activation_loss: 0.9324 - unet3plus_output_sup0_activation_dice_loss: 0.8177 - unet3plus_output_sup0_activation_iou_coeff: 0.1009 - unet3plus_output_sup0_activation_precision: 0.8744 - unet3plus_output_sup0_activation_recall: 0.7847 - unet3plus_output_sup1_activation_dice_loss: 0.9212 - unet3plus_output_sup1_activation_iou_coeff: 0.0412 - unet3plus_output_sup1_activation_precision: 0.8640 - unet3plus_output_sup1_activation_recall: 0.4308 - unet3plus_output_sup2_activation_dice_loss: 0.9749 - unet3plus_output_sup2_activation_iou_coeff: 0.0127 - unet3plus_output_sup2_activation_precision: 0.0000e+00 - unet3plus_output_sup2_activation_recall: 0.0000e+00 - unet3plus_output_sup3_activation_dice_loss: 0.9732 - unet3plus_output_sup3_activation_iou_coeff: 0.0136 - unet3plus_output_sup3_activation_precision: 0.7388 - unet3plus_output_sup3_activation_recall: 0.0222 - unet3plus_output_sup4_activation_dice_loss: 0.9751 - unet3plus_output_sup4_activation_iou_coeff: 0.0126 - unet3plus_output_sup4_activation_precision: 0.0000e+00 - unet3plus_output_sup4_activation_recall: 0.0000e+00 - unet3plus_output_final_activation_dice_loss: 0.0578 - unet3plus_output_final_activation_iou_coeff: 0.8935 - unet3plus_output_final_activation_precision: 0.9524 - unet3plus_output_final_activation_recall: 0.9514 - unet3plusconcate_output1_activation_dice_loss: 0.7954 - unet3plusconcate_output1_activation_iou_coeff: 0.1146 - unet3plusconcate_output1_activation_precision: 0.8833 - unet3plusconcate_output1_activation_recall: 0.2427 - val_loss: 7.3038 - val_unet3plus_output_sup0_activation_loss: 0.6894 - val_unet3plus_output_sup1_activation_loss: 1.1148 - val_unet3plus_output_sup2_activation_loss: 1.2754 - val_unet3plus_output_sup3_activation_loss: 1.2999 - val_unet3plus_output_sup4_activation_loss: 1.3140 - val_unet3plus_output_final_activation_loss: 0.6575 - val_unet3plusconcate_output1_activation_loss: 0.9527 - val_unet3plus_output_sup0_activation_dice_loss: 0.4298 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4384 - val_unet3plus_output_sup0_activation_precision: 0.8261 - val_unet3plus_output_sup0_activation_recall: 0.4955 - val_unet3plus_output_sup1_activation_dice_loss: 0.9112 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0467 - val_unet3plus_output_sup1_activation_precision: 0.8358 - val_unet3plus_output_sup1_activation_recall: 0.3539 - val_unet3plus_output_sup2_activation_dice_loss: 0.9743 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9726 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0139 - val_unet3plus_output_sup3_activation_precision: 0.6816 - val_unet3plus_output_sup3_activation_recall: 0.0225 - val_unet3plus_output_sup4_activation_dice_loss: 0.9746 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0129 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.4214 - val_unet3plus_output_final_activation_iou_coeff: 0.4451 - val_unet3plus_output_final_activation_precision: 0.8645 - val_unet3plus_output_final_activation_recall: 0.4804 - val_unet3plusconcate_output1_activation_dice_loss: 0.8209 - val_unet3plusconcate_output1_activation_iou_coeff: 0.1009 - val_unet3plusconcate_output1_activation_precision: 0.8202 - val_unet3plusconcate_output1_activation_recall: 0.3549\nEpoch 30/30\n350/350 [==============================] - 396s 1s/step - loss: 7.2296 - unet3plus_output_sup0_activation_loss: 1.0447 - unet3plus_output_sup1_activation_loss: 1.1948 - unet3plus_output_sup2_activation_loss: 1.3143 - unet3plus_output_sup3_activation_loss: 1.3277 - unet3plus_output_sup4_activation_loss: 1.3386 - unet3plus_output_final_activation_loss: 0.1146 - unet3plusconcate_output1_activation_loss: 0.8949 - unet3plus_output_sup0_activation_dice_loss: 0.8175 - unet3plus_output_sup0_activation_iou_coeff: 0.1011 - unet3plus_output_sup0_activation_precision: 0.8733 - unet3plus_output_sup0_activation_recall: 0.7853 - unet3plus_output_sup1_activation_dice_loss: 0.9203 - unet3plus_output_sup1_activation_iou_coeff: 0.0417 - unet3plus_output_sup1_activation_precision: 0.8559 - unet3plus_output_sup1_activation_recall: 0.4324 - unet3plus_output_sup2_activation_dice_loss: 0.9749 - unet3plus_output_sup2_activation_iou_coeff: 0.0127 - unet3plus_output_sup2_activation_precision: 0.0000e+00 - unet3plus_output_sup2_activation_recall: 0.0000e+00 - unet3plus_output_sup3_activation_dice_loss: 0.9732 - unet3plus_output_sup3_activation_iou_coeff: 0.0136 - unet3plus_output_sup3_activation_precision: 0.7443 - unet3plus_output_sup3_activation_recall: 0.0222 - unet3plus_output_sup4_activation_dice_loss: 0.9751 - unet3plus_output_sup4_activation_iou_coeff: 0.0126 - unet3plus_output_sup4_activation_precision: 7.1429e-04 - unet3plus_output_sup4_activation_recall: 3.8579e-07 - unet3plus_output_final_activation_dice_loss: 0.0583 - unet3plus_output_final_activation_iou_coeff: 0.8927 - unet3plus_output_final_activation_precision: 0.9523 - unet3plus_output_final_activation_recall: 0.9509 - unet3plusconcate_output1_activation_dice_loss: 0.7462 - unet3plusconcate_output1_activation_iou_coeff: 0.1463 - unet3plusconcate_output1_activation_precision: 0.8994 - unet3plusconcate_output1_activation_recall: 0.4619 - val_loss: 7.0218 - val_unet3plus_output_sup0_activation_loss: 0.5719 - val_unet3plus_output_sup1_activation_loss: 1.1130 - val_unet3plus_output_sup2_activation_loss: 1.2709 - val_unet3plus_output_sup3_activation_loss: 1.2922 - val_unet3plus_output_sup4_activation_loss: 1.3083 - val_unet3plus_output_final_activation_loss: 0.5624 - val_unet3plusconcate_output1_activation_loss: 0.9029 - val_unet3plus_output_sup0_activation_dice_loss: 0.3659 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4950 - val_unet3plus_output_sup0_activation_precision: 0.8453 - val_unet3plus_output_sup0_activation_recall: 0.5724 - val_unet3plus_output_sup1_activation_dice_loss: 0.9136 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0454 - val_unet3plus_output_sup1_activation_precision: 0.8665 - val_unet3plus_output_sup1_activation_recall: 0.3353 - val_unet3plus_output_sup2_activation_dice_loss: 0.9743 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0130 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9726 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0139 - val_unet3plus_output_sup3_activation_precision: 0.7044 - val_unet3plus_output_sup3_activation_recall: 0.0221 - val_unet3plus_output_sup4_activation_dice_loss: 0.9747 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0128 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.3479 - val_unet3plus_output_final_activation_iou_coeff: 0.5159 - val_unet3plus_output_final_activation_precision: 0.8767 - val_unet3plus_output_final_activation_recall: 0.5677 - val_unet3plusconcate_output1_activation_dice_loss: 0.7551 - val_unet3plusconcate_output1_activation_iou_coeff: 0.1433 - val_unet3plusconcate_output1_activation_precision: 0.8840 - val_unet3plusconcate_output1_activation_recall: 0.4688\n",
          "output_type": "stream"
        },
        {
          "execution_count": 370,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f7583ee6e10>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-14T03:13:01.947440Z",
          "iopub.execute_input": "2021-07-14T03:13:01.947793Z"
        },
        "trusted": true,
        "id": "8WmEpy8LEHSr",
        "outputId": "0e5b2766-7c1e-420e-cf34-660897a2a7b0"
      },
      "source": [
        "\n",
        "    model.fit(train_dataset,\n",
        "        validation_data=valid_dataset,\n",
        "        epochs=10,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_steps,\n",
        "        callbacks=callbacks,\n",
        "        shuffle = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/10\n350/350 [==============================] - 398s 1s/step - loss: 7.1704 - unet3plus_output_sup0_activation_loss: 1.0446 - unet3plus_output_sup1_activation_loss: 1.1913 - unet3plus_output_sup2_activation_loss: 1.3187 - unet3plus_output_sup3_activation_loss: 1.3215 - unet3plus_output_sup4_activation_loss: 1.3337 - unet3plus_output_final_activation_loss: 0.1149 - unet3plusconcate_output1_activation_loss: 0.8456 - unet3plus_output_sup0_activation_dice_loss: 0.8173 - unet3plus_output_sup0_activation_iou_coeff: 0.1012 - unet3plus_output_sup0_activation_precision: 0.8714 - unet3plus_output_sup0_activation_recall: 0.7859 - unet3plus_output_sup1_activation_dice_loss: 0.9193 - unet3plus_output_sup1_activation_iou_coeff: 0.0422 - unet3plus_output_sup1_activation_precision: 0.8520 - unet3plus_output_sup1_activation_recall: 0.4339 - unet3plus_output_sup2_activation_dice_loss: 0.9756 - unet3plus_output_sup2_activation_iou_coeff: 0.0124 - unet3plus_output_sup2_activation_precision: 0.0000e+00 - unet3plus_output_sup2_activation_recall: 0.0000e+00 - unet3plus_output_sup3_activation_dice_loss: 0.9732 - unet3plus_output_sup3_activation_iou_coeff: 0.0136 - unet3plus_output_sup3_activation_precision: 0.7408 - unet3plus_output_sup3_activation_recall: 0.0225 - unet3plus_output_sup4_activation_dice_loss: 0.9752 - unet3plus_output_sup4_activation_iou_coeff: 0.0126 - unet3plus_output_sup4_activation_precision: 0.0000e+00 - unet3plus_output_sup4_activation_recall: 0.0000e+00 - unet3plus_output_final_activation_dice_loss: 0.0584 - unet3plus_output_final_activation_iou_coeff: 0.8925 - unet3plus_output_final_activation_precision: 0.9521 - unet3plus_output_final_activation_recall: 0.9508 - unet3plusconcate_output1_activation_dice_loss: 0.6847 - unet3plusconcate_output1_activation_iou_coeff: 0.1886 - unet3plusconcate_output1_activation_precision: 0.9102 - unet3plusconcate_output1_activation_recall: 0.6353 - val_loss: 6.9774 - val_unet3plus_output_sup0_activation_loss: 0.5979 - val_unet3plus_output_sup1_activation_loss: 1.0949 - val_unet3plus_output_sup2_activation_loss: 1.2918 - val_unet3plus_output_sup3_activation_loss: 1.2857 - val_unet3plus_output_sup4_activation_loss: 1.3020 - val_unet3plus_output_final_activation_loss: 0.5614 - val_unet3plusconcate_output1_activation_loss: 0.8437 - val_unet3plus_output_sup0_activation_dice_loss: 0.3833 - val_unet3plus_output_sup0_activation_iou_coeff: 0.4764 - val_unet3plus_output_sup0_activation_precision: 0.8061 - val_unet3plus_output_sup0_activation_recall: 0.5661 - val_unet3plus_output_sup1_activation_dice_loss: 0.8943 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0560 - val_unet3plus_output_sup1_activation_precision: 0.7558 - val_unet3plus_output_sup1_activation_recall: 0.4332 - val_unet3plus_output_sup2_activation_dice_loss: 0.9762 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0120 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9719 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0142 - val_unet3plus_output_sup3_activation_precision: 0.4045 - val_unet3plus_output_sup3_activation_recall: 0.0297 - val_unet3plus_output_sup4_activation_dice_loss: 0.9747 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0128 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.3453 - val_unet3plus_output_final_activation_iou_coeff: 0.5202 - val_unet3plus_output_final_activation_precision: 0.8422 - val_unet3plus_output_final_activation_recall: 0.5936 - val_unet3plusconcate_output1_activation_dice_loss: 0.6819 - val_unet3plusconcate_output1_activation_iou_coeff: 0.1956 - val_unet3plusconcate_output1_activation_precision: 0.8470 - val_unet3plusconcate_output1_activation_recall: 0.5003\nEpoch 2/10\n350/350 [==============================] - 395s 1s/step - loss: 7.1113 - unet3plus_output_sup0_activation_loss: 1.0446 - unet3plus_output_sup1_activation_loss: 1.1882 - unet3plus_output_sup2_activation_loss: 1.3255 - unet3plus_output_sup3_activation_loss: 1.3156 - unet3plus_output_sup4_activation_loss: 1.3289 - unet3plus_output_final_activation_loss: 0.1159 - unet3plusconcate_output1_activation_loss: 0.7925 - unet3plus_output_sup0_activation_dice_loss: 0.8173 - unet3plus_output_sup0_activation_iou_coeff: 0.1012 - unet3plus_output_sup0_activation_precision: 0.8710 - unet3plus_output_sup0_activation_recall: 0.7859 - unet3plus_output_sup1_activation_dice_loss: 0.9185 - unet3plus_output_sup1_activation_iou_coeff: 0.0426 - unet3plus_output_sup1_activation_precision: 0.8452 - unet3plus_output_sup1_activation_recall: 0.4355 - unet3plus_output_sup2_activation_dice_loss: 0.9764 - unet3plus_output_sup2_activation_iou_coeff: 0.0120 - unet3plus_output_sup2_activation_precision: 0.0000e+00 - unet3plus_output_sup2_activation_recall: 0.0000e+00 - unet3plus_output_sup3_activation_dice_loss: 0.9732 - unet3plus_output_sup3_activation_iou_coeff: 0.0136 - unet3plus_output_sup3_activation_precision: 0.7490 - unet3plus_output_sup3_activation_recall: 0.0223 - unet3plus_output_sup4_activation_dice_loss: 0.9752 - unet3plus_output_sup4_activation_iou_coeff: 0.0126 - unet3plus_output_sup4_activation_precision: 0.0000e+00 - unet3plus_output_sup4_activation_recall: 0.0000e+00 - unet3plus_output_final_activation_dice_loss: 0.0589 - unet3plus_output_final_activation_iou_coeff: 0.8915 - unet3plus_output_final_activation_precision: 0.9511 - unet3plus_output_final_activation_recall: 0.9506 - unet3plusconcate_output1_activation_dice_loss: 0.6208 - unet3plusconcate_output1_activation_iou_coeff: 0.2357 - unet3plusconcate_output1_activation_precision: 0.9151 - unet3plusconcate_output1_activation_recall: 0.7540 - val_loss: 6.8546 - val_unet3plus_output_sup0_activation_loss: 0.5492 - val_unet3plus_output_sup1_activation_loss: 1.1001 - val_unet3plus_output_sup2_activation_loss: 1.2970 - val_unet3plus_output_sup3_activation_loss: 1.2775 - val_unet3plus_output_sup4_activation_loss: 1.2966 - val_unet3plus_output_final_activation_loss: 0.5491 - val_unet3plusconcate_output1_activation_loss: 0.7850 - val_unet3plus_output_sup0_activation_dice_loss: 0.3545 - val_unet3plus_output_sup0_activation_iou_coeff: 0.5092 - val_unet3plus_output_sup0_activation_precision: 0.7905 - val_unet3plus_output_sup0_activation_recall: 0.6231 - val_unet3plus_output_sup1_activation_dice_loss: 0.9033 - val_unet3plus_output_sup1_activation_iou_coeff: 0.0510 - val_unet3plus_output_sup1_activation_precision: 0.8190 - val_unet3plus_output_sup1_activation_recall: 0.3772 - val_unet3plus_output_sup2_activation_dice_loss: 0.9766 - val_unet3plus_output_sup2_activation_iou_coeff: 0.0119 - val_unet3plus_output_sup2_activation_precision: 0.0000e+00 - val_unet3plus_output_sup2_activation_recall: 0.0000e+00 - val_unet3plus_output_sup3_activation_dice_loss: 0.9726 - val_unet3plus_output_sup3_activation_iou_coeff: 0.0139 - val_unet3plus_output_sup3_activation_precision: 0.6964 - val_unet3plus_output_sup3_activation_recall: 0.0216 - val_unet3plus_output_sup4_activation_dice_loss: 0.9747 - val_unet3plus_output_sup4_activation_iou_coeff: 0.0128 - val_unet3plus_output_sup4_activation_precision: 0.0000e+00 - val_unet3plus_output_sup4_activation_recall: 0.0000e+00 - val_unet3plus_output_final_activation_dice_loss: 0.3407 - val_unet3plus_output_final_activation_iou_coeff: 0.5301 - val_unet3plus_output_final_activation_precision: 0.8338 - val_unet3plus_output_final_activation_recall: 0.6077 - val_unet3plusconcate_output1_activation_dice_loss: 0.6122 - val_unet3plusconcate_output1_activation_iou_coeff: 0.2507 - val_unet3plusconcate_output1_activation_precision: 0.8357 - val_unet3plusconcate_output1_activation_recall: 0.5640\nEpoch 3/10\n180/350 [==============>...............] - ETA: 3:03 - loss: 7.0670 - unet3plus_output_sup0_activation_loss: 1.0439 - unet3plus_output_sup1_activation_loss: 1.1856 - unet3plus_output_sup2_activation_loss: 1.3272 - unet3plus_output_sup3_activation_loss: 1.3112 - unet3plus_output_sup4_activation_loss: 1.3257 - unet3plus_output_final_activation_loss: 0.1164 - unet3plusconcate_output1_activation_loss: 0.7569 - unet3plus_output_sup0_activation_dice_loss: 0.8169 - unet3plus_output_sup0_activation_iou_coeff: 0.1015 - unet3plus_output_sup0_activation_precision: 0.8741 - unet3plus_output_sup0_activation_recall: 0.7838 - unet3plus_output_sup1_activation_dice_loss: 0.9177 - unet3plus_output_sup1_activation_iou_coeff: 0.0430 - unet3plus_output_sup1_activation_precision: 0.8470 - unet3plus_output_sup1_activation_recall: 0.4344 - unet3plus_output_sup2_activation_dice_loss: 0.9766 - unet3plus_output_sup2_activation_iou_coeff: 0.0118 - unet3plus_output_sup2_activation_precision: 0.0056 - unet3plus_output_sup2_activation_recall: 7.7700e-06 - unet3plus_output_sup3_activation_dice_loss: 0.9731 - unet3plus_output_sup3_activation_iou_coeff: 0.0137 - unet3plus_output_sup3_activation_precision: 0.7331 - unet3plus_output_sup3_activation_recall: 0.0227 - unet3plus_output_sup4_activation_dice_loss: 0.9752 - unet3plus_output_sup4_activation_iou_coeff: 0.0126 - unet3plus_output_sup4_activation_precision: 0.0000e+00 - unet3plus_output_sup4_activation_recall: 0.0000e+00 - unet3plus_output_final_activation_dice_loss: 0.0605 - unet3plus_output_final_activation_iou_coeff: 0.8909 - unet3plus_output_final_activation_precision: 0.9511 - unet3plus_output_final_activation_recall: 0.9494 - unet3plusconcate_output1_activation_dice_loss: 0.5797 - unet3plusconcate_output1_activation_iou_coeff: 0.2682 - unet3plusconcate_output1_activation_precision: 0.9171 - unet3plusconcate_output1_activation_recall: 0.8065",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hUmiujFgEHSs"
      },
      "source": [
        "    model.fit(train_dataset,\n",
        "        validation_data=valid_dataset,\n",
        "        epochs=10,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_steps=valid_steps,\n",
        "        callbacks=callbacks,\n",
        "        shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}